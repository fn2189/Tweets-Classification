{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to build the model That we are Going to use for our tweet classification python program. We will then pickle the learned model to allow the program to call it to make predictions.\n",
    "\n",
    "We start by importing all the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk import pos_tag, ne_chunk\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "import collections\n",
    "import os\n",
    "os.environ['JAVA_HOME'] = 'C:/Program Files (x86)/Java/jdk1.8.0_151'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import History, Callback, ModelCheckpoint\n",
    "from keras.utils import print_summary \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from sklearn import metrics\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/Airline-Tags.csv', header=0, encoding= \"latin_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>Tag</th>\n",
       "      <th>airline</th>\n",
       "      <th>Text</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>681448184</td>\n",
       "      <td>2/25/15 3:11</td>\n",
       "      <td>negative</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica SFO-PDX schedule is still MIA.</td>\n",
       "      <td>2/24/15 10:01</td>\n",
       "      <td>5.700000e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>681448303</td>\n",
       "      <td>2/25/15 6:08</td>\n",
       "      <td>negative</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica Hi, Virgin! I'm on hold for 40-...</td>\n",
       "      <td>2/23/15 13:35</td>\n",
       "      <td>5.700000e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>681448334</td>\n",
       "      <td>2/25/15 4:13</td>\n",
       "      <td>negative</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica you're the best!! Whenever I (b...</td>\n",
       "      <td>2/23/15 10:54</td>\n",
       "      <td>5.700000e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>681448360</td>\n",
       "      <td>2/25/15 3:30</td>\n",
       "      <td>negative</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica just landed in LAX, an hour aft...</td>\n",
       "      <td>2/23/15 9:28</td>\n",
       "      <td>5.700000e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>681448377</td>\n",
       "      <td>2/25/15 5:39</td>\n",
       "      <td>negative</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>@VirginAmerica why must a traveler miss a flig...</td>\n",
       "      <td>2/23/15 8:11</td>\n",
       "      <td>5.700000e+17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      _unit_id _last_judgment_at airline_sentiment          Tag  \\\n",
       "15   681448184      2/25/15 3:11          negative  Late Flight   \n",
       "67   681448303      2/25/15 6:08          negative  Late Flight   \n",
       "82   681448334      2/25/15 4:13          negative  Late Flight   \n",
       "93   681448360      2/25/15 3:30          negative  Late Flight   \n",
       "101  681448377      2/25/15 5:39          negative  Late Flight   \n",
       "\n",
       "            airline                                               Text  \\\n",
       "15   Virgin America      @VirginAmerica SFO-PDX schedule is still MIA.   \n",
       "67   Virgin America  @VirginAmerica Hi, Virgin! I'm on hold for 40-...   \n",
       "82   Virgin America  @VirginAmerica you're the best!! Whenever I (b...   \n",
       "93   Virgin America  @VirginAmerica just landed in LAX, an hour aft...   \n",
       "101  Virgin America  @VirginAmerica why must a traveler miss a flig...   \n",
       "\n",
       "     tweet_created      tweet_id  \n",
       "15   2/24/15 10:01  5.700000e+17  \n",
       "67   2/23/15 13:35  5.700000e+17  \n",
       "82   2/23/15 10:54  5.700000e+17  \n",
       "93    2/23/15 9:28  5.700000e+17  \n",
       "101   2/23/15 8:11  5.700000e+17  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Tag'] == 'Late Flight'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are going to build a bag-of-word classifier, it is interesting to see which type of words appear in each class to get a sense of what the important features might be and what that of preprocessing might be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@VirginAmerica SFO-PDX schedule is still MIA.',\n",
       " \"@VirginAmerica Hi, Virgin! I'm on hold for 40-50 minutes -- are there any earlier flights from LA to NYC tonight; earlier than 11:50pm?\",\n",
       " \"@VirginAmerica you're the best!! Whenever I (begrudgingly) use any other airline I'm delayed and Late Flight :(\",\n",
       " '@VirginAmerica just landed in LAX, an hour after I should of been here. Your no Late Flight bag check is not business travel friendly #nomorevirgin',\n",
       " '@VirginAmerica why must a traveler miss a flight to Late Flight check a bag?  I missed my morning appointments and you lost my business. #sfo2lax']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data[data['Tag'] == 'Late Flight'].head()['Text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with multiple classes but since we are only interested in differenciating between tweets that are related to a late flight and tweets that are not, we are going to merge all the other classes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Late_Flight'] = data['Tag'].apply(lambda x: 1 if x == 'Late Flight' else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to create an efficient bag-of-word model, we need some preprocessing. This consist of removing useless information like punctiation, useless words and also tokenizing thewords. good exampleof meaningful tokenization include dates, times or usernames, for which each instance is probably going to be different but carry the same exact semantical meaning. Without reconciling them, we would not be able to learn anything forthem. This also allows us to reduce the size of the feature space which usually allows us to reduce the variance of a learned model.\n",
    "\n",
    "To leverage the full extent of scikit learn tools for cross-validation and parameter tuning, it is useful to be able to include all of our preprocessing stepin the pipeline that is passed to the cross-validator. \n",
    "\n",
    "To do so, sciki-learn provides us the means to create our custom scikit-learn preprocessor. It requires at least a fit and a transform method. The preprocessor used here is taken from: https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html. I just added my custom regex-based tokenizationsteps to fit this particular dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, stopwords=None, punct=None,\n",
    "                 lower=True, strip=True, remove_stopwords=True):\n",
    "        self.lower      = lower\n",
    "        self.strip      = strip\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.stopwords  = stopwords or set(sw.words('english'))\n",
    "        self.punct      = punct or set(string.punctuation)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.dates = [\n",
    "        'mon(?![a-z])', 'monday',\n",
    "        'tues(?![a-z])', 'tuesday',\n",
    "        'wednesday',\n",
    "        'thur(?![a-z])', 'thurs', 'thursday',\n",
    "        'fri(?![a-z])', 'friday',\n",
    "        'saturday',\n",
    "        'sunday'\n",
    "    ]\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def join(self, X):\n",
    "        return [\" \".join(doc) for doc in X]\n",
    "\n",
    "    def transform(self, X):\n",
    "        l =  [\n",
    "            list(self.tokenize(doc)) for doc in X\n",
    "        ]\n",
    "        return self.join(l)\n",
    "\n",
    "    def tokenize(self, document):\n",
    "        # Break the document into sentences\n",
    "        for sent in sent_tokenize(document):\n",
    "            # Break the sentence into part of speech tagged tokens\n",
    "            sent_acc = self.replace_accounts(sent)\n",
    "            sent_links = self.replace_links(sent_acc)\n",
    "            sent_cities = self.replace_cities(sent_links)\n",
    "            sent_times = self.replace_times(sent_cities)\n",
    "            sent_delays = self.replace_delays(sent_times)\n",
    "            sent_dates = self.replace_dates(sent_delays)\n",
    "            sent_hashtags = self.replace_hashtags(sent_dates)\n",
    "            sent_flight_numbers = self.replace_flight_numbers(sent_hashtags)\n",
    "            for token, tag in pos_tag(wordpunct_tokenize(sent_flight_numbers)):\n",
    "                # Apply preprocessing to the token\n",
    "                token = token.lower() if self.lower else token\n",
    "                token = token.strip() if self.strip else token\n",
    "                token = token.strip('_') if self.strip else token\n",
    "                token = token.strip('*') if self.strip else token\n",
    "    \n",
    "                # If stopword, ignore token and continue\n",
    "                if self.remove_stopwords:\n",
    "                    if token in self.stopwords:\n",
    "                        continue\n",
    "\n",
    "                # If punctuation, ignore token and continue\n",
    "                if all(char in self.punct for char in token):\n",
    "                    continue\n",
    "\n",
    "                # Lemmatize the token and yield\n",
    "                lemma = self.lemmatize(token, tag)\n",
    "                yield lemma\n",
    "                \n",
    "    def replace_accounts(self, sen):\n",
    "        return re.sub('@[^ ]+', 'TOPERSON0', sen)\n",
    "    \n",
    "    def replace_links(self, sen):\n",
    "        return re.sub('http[s]*[:][/]{2}[^ ]+', 'LINK0', sen)\n",
    "    \n",
    "    def replace_cities(self, sen):\n",
    "        return re.sub('(?<![A-Z0-9])([A-Z]{2,3})(?![A-Z0-9])', 'CITY0', sen)\n",
    "    \n",
    "    def replace_times(self, sen):\n",
    "        return re.sub('[0-9]{1,2}[:][0-9]{1,2}', ' TIME0 ', sen)\n",
    "    def replace_delays(self, sen):\n",
    "        return re.sub('[0-9]{1,2}[ ]*(minutes|minute|mins|min|hours|hours|hrs|hr)', 'DELAY0', sen)\n",
    "    \n",
    "    \n",
    "    def replace_dates(self, sen):\n",
    "        return re.sub('('+'|'.join(self.dates)+')', 'DATE0', sen, flags=re.IGNORECASE)\n",
    "    \n",
    "    def replace_hashtags(self, sen):\n",
    "        return re.sub('#[a-zA-Z0-9]+', 'HASHTAG0', sen)\n",
    "    \n",
    "    def replace_flight_numbers(self, sen):\n",
    "        return re.sub('([a-zA-Z]{2})*[0-9]{4}', 'FLIGHTNUMBER0', sen)\n",
    "    \n",
    "\n",
    "    def lemmatize(self, token, tag):\n",
    "        tag = {\n",
    "            'N': wn.NOUN,\n",
    "            'V': wn.VERB,\n",
    "            'R': wn.ADV,\n",
    "            'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return self.lemmatizer.lemmatize(token, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Once that is done, we can start building a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines\n",
    "\n",
    "In order to know what performancce we should expect from our learn model, It is useful to use to create a dummy model to get a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data['Late_Flight']#.astype(np.float64)\n",
    "X = data['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy accuracy:  0.8862704918032787\n"
     ]
    }
   ],
   "source": [
    "#Predict Majority class\n",
    "print('Dummy accuracy: ', 1- np.mean(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that because of the high imbalances of classes, thedummy classifier gets a very high accuracy. However, it is not able to detect any positive example so it is useless. In order to adress that, We are going to optimize a different metric when tuning the parameters, the roc_auc score for example, that is more representative of the ability of the classifier to accurately differenciate between positive and negative examples.\n",
    "\n",
    "We are also going to weight our examples so that both classes have equal weights.\n",
    "\n",
    "Let's now see what perfomance we get with a vanilla logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "processor = NLTKPreprocessor(remove_stopwords=False)\n",
    "vect = CountVectorizer(analyzer='word')\n",
    "logit = LogisticRegression(class_weight='balanced')\n",
    "pipe= Pipeline([('processor', processor), ('vect', vect) , ('logit', logit)])\n",
    "#pipe.fit(X_train, y_train)\n",
    "#y_hat = pipe.predict(X_test)\n",
    "#score = f1_score(y_test, y_hat, average='macro')\n",
    "scores = cross_val_score(pipe, X, y, cv=10, scoring='accuracy' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.90784983, 0.87986348, 0.89283276, 0.92423208, 0.92969283,\n",
       "       0.89747095, 0.88311688, 0.87833219, 0.91660971, 0.8885851 ])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8998585822296977, 0.01776017577142671)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Our vanilla model performs definitely better that the dummy classifier ( whose roc_auc_score would be .5). Let's have a look at the important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('processor', NLTKPreprocessor(lower=True,\n",
       "         punct={';', '&', ')', '~', ':', '`', '#', '^', '/', '<', '?', '%', '*', '{', '_', '-', ',', '.', '}', '>', '+', '@', '\"', ']', '\\\\', '(', '$', '=', '|', '[', '!', \"'\"},\n",
       "         remove_stopwords=False,\n",
       "         stopwords={'that', 'once', 'ot...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y)\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_important_features(coef, feature_names, top_n=20, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    inds = np.argsort(coef)\n",
    "    low = inds[:top_n]\n",
    "    high = inds[-top_n:]\n",
    "    important = np.hstack([low, high])\n",
    "    myrange = range(len(important))\n",
    "\n",
    "    ax.bar(myrange, coef[important])\n",
    "    ax.set_xticks(myrange)\n",
    "    ax.set_xticklabels(feature_names[important], rotation=60, ha=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Words importance\n",
    "w = pipe.named_steps['logit'].coef_\n",
    "#w.shape\n",
    "features = pipe.named_steps['vect'].get_feature_names()\n",
    "\"\"\"\n",
    "fig, axarr = plt.subplots(5,3, figsize=(15,15))\n",
    "for i in range(w.shape[0]):\n",
    "    plot_important_features(w[i,:].ravel(), np.array(features), top_n=10, ax=axarr[i%5,i//5])\n",
    "    axarr[i%5,i//5].set_title(i)\n",
    "fig.tight_layout()\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "ax = plt.gca()\n",
    "plot_important_features(w[0,:].ravel(), np.array(features), top_n=20, ax=ax)\n",
    "ax.set_title(0)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, we can see that words delay, delayed and are the most positively important. \n",
    "Let's now proceed to some parameter tuning. \n",
    "We have 3 main parts for our model (the 3 steps of our pipeline). Ideally, with infinite time and processing power, we would tune the 3 parts jointly. However, to limit the time, we will tune them separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started by refining my preprocessor (skipped here) and then decided to see in reducing the feature space by discarding nfrequent word would improve performance by reducing overfitting (Did not work)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning the min count of words  (feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.91332, std: 0.00690, params: {'vect__min_df': 1},\n",
       "  mean: 0.91185, std: 0.00614, params: {'vect__min_df': 3},\n",
       "  mean: 0.91146, std: 0.00577, params: {'vect__min_df': 5}],\n",
       " {'vect__min_df': 1},\n",
       " 0.9133238440752892)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = NLTKPreprocessor()\n",
    "vect = CountVectorizer()\n",
    "logit = LogisticRegression(class_weight='balanced')\n",
    "pipe= Pipeline([('processor', processor), ('vect', vect) , ('logit', logit)])\n",
    "param_grid = {\n",
    "    #'logit__C':[.1, 1, 10, 10],\n",
    "    'vect__min_df':[1, 3, 5]\n",
    "}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='roc_auc')\n",
    "grid.fit(X, y)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was tuning different models as the last step of my pipeline to try to improve performance. A lot more work could be done on the preprocessor to better tokenize words but my main focus here was really playing wit the models ad their parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "I started with the logisting regression for which I tried L1 and L2 penalty with different Cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.77134, std: 0.01751, params: {'logit__C': 0.1, 'logit__penalty': 'l1'},\n",
       "  mean: 0.78004, std: 0.01981, params: {'logit__C': 0.1, 'logit__penalty': 'l2'},\n",
       "  mean: 0.78484, std: 0.02061, params: {'logit__C': 1, 'logit__penalty': 'l1'},\n",
       "  mean: 0.78500, std: 0.01910, params: {'logit__C': 1, 'logit__penalty': 'l2'},\n",
       "  mean: 0.75120, std: 0.01368, params: {'logit__C': 10, 'logit__penalty': 'l1'},\n",
       "  mean: 0.76528, std: 0.02128, params: {'logit__C': 10, 'logit__penalty': 'l2'},\n",
       "  mean: 0.71575, std: 0.01251, params: {'logit__C': 100, 'logit__penalty': 'l1'},\n",
       "  mean: 0.72774, std: 0.01732, params: {'logit__C': 100, 'logit__penalty': 'l2'}],\n",
       " {'logit__C': 1, 'logit__penalty': 'l2'},\n",
       " 0.7849957504260902)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "processor = NLTKPreprocessor()\n",
    "vect = CountVectorizer(min_df=1)\n",
    "logit = LogisticRegression(class_weight='balanced')\n",
    "pipe= Pipeline([('processor', processor), ('vect', vect) , ('logit', logit)])\n",
    "param_grid = {\n",
    "    #'vect__min_df':[1, 3, 5],\n",
    "    'logit__penalty':['l1', 'l2'],\n",
    "    'logit__C':[.1, 1, 10, 100],\n",
    "    \n",
    "}\n",
    "grid = GridSearchCV(pipe, param_grid, cv=5, scoring='roc_auc')\n",
    "grid.fit(X, y)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xgboost\n",
    "\n",
    "The Xgboost algorithm is very powerful but it comes with a lot of parameters. I have found that tuning them can really goes a long way.\n",
    "Since this algorithm can be overwhelming at first, I would advice you to go to this very good tutorial : https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "whose guidelines  taught made a lot of sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "processor = NLTKPreprocessor(remove_stopwords=False)\n",
    "vect = CountVectorizer(min_df=1)\n",
    "pipe= Pipeline([('processor', processor), ('vect', vect) ])\n",
    "X_vect = pipe.fit_transform(X, y)#.astype(np.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_vect = X_vect.astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_resume(X_vect, pipe, ascending = False, n = None):\n",
    "    feature_names = pipe.named_steps['vect'].get_feature_names()\n",
    "\n",
    "    resume = pd.DataFrame(columns = feature_names, data = X_vect.toarray()).sum()\n",
    "\n",
    "    if(n):\n",
    "        return resume.sort_values(ascending = ascending)[:n]\n",
    "\n",
    "    return resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toperson0    16475\n",
       "to            8621\n",
       "be            8271\n",
       "the           6015\n",
       "city0         5195\n",
       "flight        4632\n",
       "you           4349\n",
       "for           3979\n",
       "on            3792\n",
       "and           3709\n",
       "hashtag0      3276\n",
       "my            3259\n",
       "have          2706\n",
       "in            2584\n",
       "it            2528\n",
       "of            2113\n",
       "get           2066\n",
       "me            1894\n",
       "your          1743\n",
       "that          1733\n",
       "can           1657\n",
       "do            1589\n",
       "not           1532\n",
       "with          1523\n",
       "delay0        1483\n",
       "at            1482\n",
       "no            1463\n",
       "this          1393\n",
       "we            1284\n",
       "but           1237\n",
       "from          1201\n",
       "link0         1154\n",
       "thanks        1071\n",
       "now           1026\n",
       "service       1002\n",
       "an             980\n",
       "time           975\n",
       "just           974\n",
       "customer       938\n",
       "help           936\n",
       "so             884\n",
       "delay          881\n",
       "what           816\n",
       "call           789\n",
       "will           771\n",
       "they           765\n",
       "bag            756\n",
       "fly            753\n",
       "go             751\n",
       "wait           744\n",
       "dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_resume(X_vect, pipe, n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.787326+0.0321552\ttrain-error:0.20123+0.0717506\ttest-auc:0.754716+0.0314314\ttest-error:0.215915+0.0769983\n",
      "[1]\ttrain-auc:0.813246+0.0298237\ttrain-error:0.161373+0.0726258\ttest-auc:0.791443+0.0235292\ttest-error:0.172678+0.0777422\n",
      "[2]\ttrain-auc:0.851325+0.0118869\ttrain-error:0.141189+0.0525134\ttest-auc:0.830985+0.00235097\ttest-error:0.15123+0.0546737\n",
      "[3]\ttrain-auc:0.864166+0.0116582\ttrain-error:0.180977+0.0484933\ttest-auc:0.838182+0.00621616\ttest-error:0.195014+0.0495841\n",
      "[4]\ttrain-auc:0.867922+0.0168673\ttrain-error:0.192555+0.056635\ttest-auc:0.842977+0.00948308\ttest-error:0.206489+0.0599005\n",
      "[5]\ttrain-auc:0.872589+0.016542\ttrain-error:0.157992+0.0605423\ttest-auc:0.848001+0.00909308\ttest-error:0.170902+0.0623119\n",
      "[6]\ttrain-auc:0.88129+0.0089077\ttrain-error:0.179542+0.0535956\ttest-auc:0.85839+0.00625447\ttest-error:0.193716+0.0564505\n",
      "[7]\ttrain-auc:0.88721+0.00596501\ttrain-error:0.177459+0.0561925\ttest-auc:0.864093+0.004496\ttest-error:0.192691+0.0569827\n",
      "[8]\ttrain-auc:0.891504+0.00359823\ttrain-error:0.122165+0.00489691\ttest-auc:0.866392+0.00373552\ttest-error:0.132036+0.00544322\n",
      "[9]\ttrain-auc:0.893689+0.00289631\ttrain-error:0.116257+0.011218\ttest-auc:0.868643+0.00568387\ttest-error:0.125888+0.0099048\n",
      "[10]\ttrain-auc:0.898495+0.00147195\ttrain-error:0.112227+0.00314043\ttest-auc:0.875888+0.00271183\ttest-error:0.123839+0.00385339\n",
      "[11]\ttrain-auc:0.899132+0.000556767\ttrain-error:0.112466+0.00226072\ttest-auc:0.876742+0.00196835\ttest-error:0.124249+0.00318049\n",
      "[12]\ttrain-auc:0.899973+0.00140912\ttrain-error:0.114481+0.00460197\ttest-auc:0.878009+0.00294751\ttest-error:0.125341+0.00491357\n",
      "[13]\ttrain-auc:0.903784+0.00193783\ttrain-error:0.11291+0.00608109\ttest-auc:0.881737+0.00425017\ttest-error:0.123361+0.0068718\n",
      "[14]\ttrain-auc:0.906071+0.00150197\ttrain-error:0.10929+0.00587375\ttest-auc:0.883393+0.00407269\ttest-error:0.121175+0.00677859\n",
      "[15]\ttrain-auc:0.907044+0.00173255\ttrain-error:0.108709+0.00602482\ttest-auc:0.885106+0.00319298\ttest-error:0.121107+0.00623585\n",
      "[16]\ttrain-auc:0.908584+0.00213015\ttrain-error:0.10789+0.00592911\ttest-auc:0.886048+0.00333033\ttest-error:0.119262+0.00659159\n",
      "[17]\ttrain-auc:0.910108+0.00230099\ttrain-error:0.106284+0.00671053\ttest-auc:0.887436+0.00377126\ttest-error:0.117213+0.00562169\n",
      "[18]\ttrain-auc:0.91195+0.00248807\ttrain-error:0.105464+0.00640782\ttest-auc:0.889081+0.00341062\ttest-error:0.116598+0.00527247\n",
      "[19]\ttrain-auc:0.913256+0.00218356\ttrain-error:0.104132+0.00801652\ttest-auc:0.89022+0.00283937\ttest-error:0.114412+0.00845628\n",
      "[20]\ttrain-auc:0.914769+0.0022669\ttrain-error:0.103825+0.00807402\ttest-auc:0.892161+0.00315601\ttest-error:0.113934+0.00904916\n",
      "[21]\ttrain-auc:0.915876+0.00230368\ttrain-error:0.101913+0.00581391\ttest-auc:0.893607+0.00209331\ttest-error:0.111066+0.00565158\n",
      "[22]\ttrain-auc:0.91689+0.00187362\ttrain-error:0.101674+0.00553041\ttest-auc:0.894852+0.00261828\ttest-error:0.112158+0.00399245\n",
      "[23]\ttrain-auc:0.918371+0.000962499\ttrain-error:0.101264+0.00604259\ttest-auc:0.896506+0.00376732\ttest-error:0.111953+0.00477433\n",
      "[24]\ttrain-auc:0.920704+0.00198472\ttrain-error:0.101127+0.00594873\ttest-auc:0.897574+0.00307204\ttest-error:0.11332+0.00462183\n",
      "[25]\ttrain-auc:0.921942+0.00156036\ttrain-error:0.10082+0.00680001\ttest-auc:0.898834+0.00376532\ttest-error:0.111817+0.007211\n",
      "[26]\ttrain-auc:0.922842+0.00164142\ttrain-error:0.0993853+0.00586065\ttest-auc:0.899856+0.00354345\ttest-error:0.110724+0.0059448\n",
      "[27]\ttrain-auc:0.923288+0.00160665\ttrain-error:0.098873+0.00645641\ttest-auc:0.900226+0.00398345\ttest-error:0.110519+0.0057091\n",
      "[28]\ttrain-auc:0.924501+0.000924047\ttrain-error:0.0975067+0.00574076\ttest-auc:0.900813+0.00454707\ttest-error:0.11127+0.00526717\n",
      "[29]\ttrain-auc:0.925617+0.000830205\ttrain-error:0.0985997+0.00497923\ttest-auc:0.900953+0.00390429\ttest-error:0.112295+0.0032958\n",
      "[30]\ttrain-auc:0.927038+0.00098977\ttrain-error:0.0983947+0.00544282\ttest-auc:0.901796+0.00306176\ttest-error:0.112295+0.00354142\n",
      "[31]\ttrain-auc:0.928661+0.00101242\ttrain-error:0.0983607+0.00586109\ttest-auc:0.903036+0.00394239\ttest-error:0.111544+0.00280655\n",
      "[32]\ttrain-auc:0.930441+0.000646781\ttrain-error:0.0981557+0.00567581\ttest-auc:0.904315+0.00344621\ttest-error:0.110929+0.00300419\n",
      "[33]\ttrain-auc:0.931148+0.000353874\ttrain-error:0.0965503+0.00588248\ttest-auc:0.904998+0.00328872\ttest-error:0.10929+0.00310926\n",
      "[34]\ttrain-auc:0.931633+0.000503931\ttrain-error:0.0959017+0.0057235\ttest-auc:0.905397+0.00355278\ttest-error:0.109153+0.00357811\n",
      "[35]\ttrain-auc:0.932383+0.000355851\ttrain-error:0.0953893+0.0058482\ttest-auc:0.905902+0.00360334\ttest-error:0.108606+0.00285898\n",
      "[36]\ttrain-auc:0.933491+0.000561915\ttrain-error:0.0949453+0.00474301\ttest-auc:0.906006+0.00317606\ttest-error:0.108538+0.00261513\n",
      "[37]\ttrain-auc:0.934485+0.00084488\ttrain-error:0.0941597+0.00481745\ttest-auc:0.906668+0.00304874\ttest-error:0.107855+0.00251177\n",
      "[38]\ttrain-auc:0.935831+0.000535115\ttrain-error:0.094399+0.00410546\ttest-auc:0.907485+0.00295495\ttest-error:0.108265+0.00160757\n",
      "[39]\ttrain-auc:0.936394+0.000835249\ttrain-error:0.0942283+0.0043713\ttest-auc:0.908182+0.00310702\ttest-error:0.108606+0.000885238\n",
      "[40]\ttrain-auc:0.937109+0.000921042\ttrain-error:0.093067+0.00397459\ttest-auc:0.908649+0.00338094\ttest-error:0.108197+0.00176266\n",
      "[41]\ttrain-auc:0.937896+0.00100468\ttrain-error:0.0935113+0.00441591\ttest-auc:0.908834+0.00311605\ttest-error:0.108265+0.00237413\n",
      "[42]\ttrain-auc:0.938548+0.00125351\ttrain-error:0.0928963+0.00357419\ttest-auc:0.909586+0.00289112\ttest-error:0.108675+0.000754344\n",
      "[43]\ttrain-auc:0.939569+0.0011218\ttrain-error:0.0927937+0.00372735\ttest-auc:0.910024+0.00277335\ttest-error:0.108333+0.000587438\n",
      "[44]\ttrain-auc:0.939946+0.000960478\ttrain-error:0.0925207+0.00492949\ttest-auc:0.910516+0.00292819\ttest-error:0.10765+0.00125582\n",
      "[45]\ttrain-auc:0.940239+0.00101492\ttrain-error:0.0919057+0.00449256\ttest-auc:0.911084+0.00314458\ttest-error:0.106899+0.00125588\n",
      "[46]\ttrain-auc:0.941061+0.000596756\ttrain-error:0.091496+0.004504\ttest-auc:0.911435+0.00354111\ttest-error:0.106694+0.00151835\n",
      "[47]\ttrain-auc:0.941809+0.000410994\ttrain-error:0.0913593+0.00460038\ttest-auc:0.911543+0.00367316\ttest-error:0.107924+0.00126664\n",
      "[48]\ttrain-auc:0.942585+0.00078187\ttrain-error:0.0905397+0.00393692\ttest-auc:0.911764+0.00385183\ttest-error:0.107104+0.000842091\n",
      "[49]\ttrain-auc:0.943432+0.000778518\ttrain-error:0.0900957+0.00396833\ttest-auc:0.912002+0.00385006\ttest-error:0.106489+0.000980303\n",
      "[50]\ttrain-auc:0.944057+0.00075303\ttrain-error:0.08972+0.00364679\ttest-auc:0.912576+0.00395074\ttest-error:0.105738+0.000931576\n",
      "[51]\ttrain-auc:0.944907+0.000653369\ttrain-error:0.0892417+0.00337045\ttest-auc:0.913042+0.00388123\ttest-error:0.105942+0.000729132\n",
      "[52]\ttrain-auc:0.945738+0.000854209\ttrain-error:0.0889003+0.00373415\ttest-auc:0.913791+0.00373466\ttest-error:0.105259+0.0010085\n",
      "[53]\ttrain-auc:0.946322+0.000923514\ttrain-error:0.088832+0.00366957\ttest-auc:0.914046+0.00352983\ttest-error:0.106011+0.00118707\n",
      "[54]\ttrain-auc:0.94677+0.000946952\ttrain-error:0.0883197+0.00370648\ttest-auc:0.914051+0.00341257\ttest-error:0.106421+0.000510914\n",
      "[55]\ttrain-auc:0.947685+0.000758117\ttrain-error:0.088149+0.00344626\ttest-auc:0.914056+0.00317704\ttest-error:0.106489+0.000482718\n",
      "[56]\ttrain-auc:0.948562+0.000531986\ttrain-error:0.0878413+0.00316282\ttest-auc:0.914334+0.0033463\ttest-error:0.10683+0.00025568\n",
      "[57]\ttrain-auc:0.94913+0.000695915\ttrain-error:0.0878757+0.00305152\ttest-auc:0.914863+0.0033558\ttest-error:0.106626+0.000510914\n",
      "[58]\ttrain-auc:0.949959+0.000760337\ttrain-error:0.0878073+0.00314907\ttest-auc:0.914982+0.00371547\ttest-error:0.106762+0.000442851\n",
      "[59]\ttrain-auc:0.950499+0.000809896\ttrain-error:0.087329+0.00336753\ttest-auc:0.914863+0.0037274\ttest-error:0.106626+0.000921472\n",
      "[60]\ttrain-auc:0.951314+0.000783326\ttrain-error:0.0871927+0.00378184\ttest-auc:0.915284+0.0039782\ttest-error:0.106352+0.000603051\n",
      "[61]\ttrain-auc:0.951928+0.00104642\ttrain-error:0.08709+0.00362347\ttest-auc:0.915391+0.00383092\ttest-error:0.106216+0.00038608\n"
     ]
    }
   ],
   "source": [
    "early_stopping = 10\n",
    "params = {'eta': 0.1, 'max_depth': 5, 'subsample': 0.7, 'colsample_bytree': 0.7, \n",
    "          'objective': 'binary:logistic', 'seed': 99, 'silent': 1, 'eval_metric':['error', 'auc'], 'nthread':1,\n",
    "         'scale_pos_weight': 1/np.mean(y)}\n",
    "\n",
    "xg_train = xgb.DMatrix(X_vect, label=y)\n",
    "cv = xgb.cv(params, xg_train, 5000, folds=5, early_stopping_rounds=early_stopping, verbose_eval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.90375, std: 0.01353, params: {'max_depth': 3, 'min_child_weight': 1},\n",
       "  mean: 0.90313, std: 0.01309, params: {'max_depth': 3, 'min_child_weight': 3},\n",
       "  mean: 0.90457, std: 0.01322, params: {'max_depth': 3, 'min_child_weight': 5},\n",
       "  mean: 0.91170, std: 0.01155, params: {'max_depth': 5, 'min_child_weight': 1},\n",
       "  mean: 0.91062, std: 0.01240, params: {'max_depth': 5, 'min_child_weight': 3},\n",
       "  mean: 0.91403, std: 0.01185, params: {'max_depth': 5, 'min_child_weight': 5},\n",
       "  mean: 0.91308, std: 0.01264, params: {'max_depth': 7, 'min_child_weight': 1},\n",
       "  mean: 0.91349, std: 0.01235, params: {'max_depth': 7, 'min_child_weight': 3},\n",
       "  mean: 0.91512, std: 0.01200, params: {'max_depth': 7, 'min_child_weight': 5},\n",
       "  mean: 0.91322, std: 0.01042, params: {'max_depth': 9, 'min_child_weight': 1},\n",
       "  mean: 0.91526, std: 0.01034, params: {'max_depth': 9, 'min_child_weight': 3},\n",
       "  mean: 0.91642, std: 0.01233, params: {'max_depth': 9, 'min_child_weight': 5}],\n",
       " {'max_depth': 9, 'min_child_weight': 5},\n",
       " 0.9164198880961887)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1st round of tuning\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "\n",
    "estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=61, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.7, colsample_bytree=0.7,\n",
    " objective= 'binary:logistic', nthread=-1, seed=27, scale_pos_weight = 1/np.mean(y))\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator, param_grid = param_test1, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=5 )\n",
    "\n",
    "grid.fit(X_vect,y)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.91729, std: 0.01014, params: {'max_depth': 10, 'min_child_weight': 5},\n",
       "  mean: 0.91760, std: 0.01168, params: {'max_depth': 10, 'min_child_weight': 7},\n",
       "  mean: 0.91797, std: 0.01042, params: {'max_depth': 10, 'min_child_weight': 9},\n",
       "  mean: 0.91625, std: 0.01050, params: {'max_depth': 12, 'min_child_weight': 5},\n",
       "  mean: 0.91837, std: 0.01106, params: {'max_depth': 12, 'min_child_weight': 7},\n",
       "  mean: 0.91845, std: 0.00946, params: {'max_depth': 12, 'min_child_weight': 9},\n",
       "  mean: 0.91650, std: 0.00896, params: {'max_depth': 14, 'min_child_weight': 5},\n",
       "  mean: 0.91785, std: 0.01087, params: {'max_depth': 14, 'min_child_weight': 7},\n",
       "  mean: 0.91856, std: 0.01157, params: {'max_depth': 14, 'min_child_weight': 9}],\n",
       " {'max_depth': 14, 'min_child_weight': 9},\n",
       " 0.9185571698866497)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best params both borders so we search higer\n",
    "param_test1 = {\n",
    " 'max_depth':[10, 12, 14],\n",
    " 'min_child_weight':[5, 7, 9]\n",
    "}\n",
    "\n",
    "estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=61, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.7, colsample_bytree=0.7,\n",
    " objective= 'binary:logistic', nthread=-1, seed=27, scale_pos_weight = 1/np.mean(y))\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator, param_grid = param_test1, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=5 )\n",
    "\n",
    "grid.fit(X_vect,y)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.91876, std: 0.01052, params: {'max_depth': 14, 'min_child_weight': 11},\n",
       "  mean: 0.91776, std: 0.00823, params: {'max_depth': 14, 'min_child_weight': 15},\n",
       "  mean: 0.91779, std: 0.01103, params: {'max_depth': 14, 'min_child_weight': 19},\n",
       "  mean: 0.91799, std: 0.00933, params: {'max_depth': 15, 'min_child_weight': 11},\n",
       "  mean: 0.91773, std: 0.00985, params: {'max_depth': 15, 'min_child_weight': 15},\n",
       "  mean: 0.91776, std: 0.00958, params: {'max_depth': 15, 'min_child_weight': 19},\n",
       "  mean: 0.91766, std: 0.00985, params: {'max_depth': 16, 'min_child_weight': 11},\n",
       "  mean: 0.91845, std: 0.00909, params: {'max_depth': 16, 'min_child_weight': 15},\n",
       "  mean: 0.91825, std: 0.01065, params: {'max_depth': 16, 'min_child_weight': 19}],\n",
       " {'max_depth': 14, 'min_child_weight': 11},\n",
       " 0.918757717254827)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best params both borders so we search higer\n",
    "param_test1 = {\n",
    " 'max_depth':[14, 15, 16],\n",
    " 'min_child_weight':[11, 15, 19]\n",
    "}\n",
    "\n",
    "estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=61, max_depth=5,\n",
    " min_child_weight=1, gamma=0, subsample=0.7, colsample_bytree=0.7,\n",
    " objective= 'binary:logistic', nthread=-1, seed=27, scale_pos_weight = 1/np.mean(y))\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator, param_grid = param_test1, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=5 )\n",
    "\n",
    "grid.fit(X_vect,y)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.91920, std: 0.00866, params: {'min_child_weight': 13},\n",
       "  mean: 0.91905, std: 0.00843, params: {'min_child_weight': 14},\n",
       "  mean: 0.91776, std: 0.00823, params: {'min_child_weight': 15}],\n",
       " {'min_child_weight': 13},\n",
       " 0.91920035642579)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finetuning min_child_weight by reducing steps\n",
    "param_test1 = {\n",
    " \n",
    " 'min_child_weight':[13, 14, 15]\n",
    "}\n",
    "\n",
    "estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=61, max_depth=14,\n",
    " min_child_weight=1, gamma=0, subsample=0.7, colsample_bytree=0.7,\n",
    " objective= 'binary:logistic', nthread=-1, seed=27, scale_pos_weight = 1/np.mean(y))\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator, param_grid = param_test1, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=5 )\n",
    "\n",
    "grid.fit(X_vect,y)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning gamma (minimun loss reduction to make a split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.91984, std: 0.00856, params: {'gamma': 10},\n",
       "  mean: 0.92033, std: 0.00923, params: {'gamma': 20},\n",
       "  mean: 0.91254, std: 0.01211, params: {'gamma': 50},\n",
       "  mean: 0.89936, std: 0.01454, params: {'gamma': 100}],\n",
       " {'gamma': 20},\n",
       " 0.9203299253010236)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    " \n",
    " 'gamma': [10, 20, 50, 100]\n",
    "}\n",
    "\n",
    "estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=61, max_depth=14,\n",
    " min_child_weight=13, gamma=0, subsample=0.7, colsample_bytree=0.7,\n",
    " objective= 'binary:logistic', nthread=-1, seed=27, scale_pos_weight = 1/np.mean(y))\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator, param_grid = param_test1, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=5 )\n",
    "\n",
    "grid.fit(X_vect,y)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning subsample and colsample_bytree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.91891, std: 0.01010, params: {'colsample_bytree': 0.6, 'subsample': 0.6},\n",
       "  mean: 0.92095, std: 0.01096, params: {'colsample_bytree': 0.6, 'subsample': 0.7},\n",
       "  mean: 0.92105, std: 0.00957, params: {'colsample_bytree': 0.6, 'subsample': 0.8},\n",
       "  mean: 0.92219, std: 0.01019, params: {'colsample_bytree': 0.6, 'subsample': 0.9},\n",
       "  mean: 0.91943, std: 0.01013, params: {'colsample_bytree': 0.7, 'subsample': 0.6},\n",
       "  mean: 0.92033, std: 0.00923, params: {'colsample_bytree': 0.7, 'subsample': 0.7},\n",
       "  mean: 0.92105, std: 0.00917, params: {'colsample_bytree': 0.7, 'subsample': 0.8},\n",
       "  mean: 0.92134, std: 0.00994, params: {'colsample_bytree': 0.7, 'subsample': 0.9},\n",
       "  mean: 0.91916, std: 0.00885, params: {'colsample_bytree': 0.8, 'subsample': 0.6},\n",
       "  mean: 0.91988, std: 0.00997, params: {'colsample_bytree': 0.8, 'subsample': 0.7},\n",
       "  mean: 0.92013, std: 0.00958, params: {'colsample_bytree': 0.8, 'subsample': 0.8},\n",
       "  mean: 0.92079, std: 0.00878, params: {'colsample_bytree': 0.8, 'subsample': 0.9},\n",
       "  mean: 0.91832, std: 0.00942, params: {'colsample_bytree': 0.9, 'subsample': 0.6},\n",
       "  mean: 0.91948, std: 0.00978, params: {'colsample_bytree': 0.9, 'subsample': 0.7},\n",
       "  mean: 0.91998, std: 0.00864, params: {'colsample_bytree': 0.9, 'subsample': 0.8},\n",
       "  mean: 0.92055, std: 0.00773, params: {'colsample_bytree': 0.9, 'subsample': 0.9}],\n",
       " {'colsample_bytree': 0.6, 'subsample': 0.9},\n",
       " 0.9221903984909765)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    " \n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "\n",
    "estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=61, max_depth=14,\n",
    " min_child_weight=13, gamma=20, subsample=0.7, colsample_bytree=0.7,\n",
    " objective= 'binary:logistic', nthread=-1, seed=27, scale_pos_weight = 1/np.mean(y))\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator, param_grid = param_test1, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=5 )\n",
    "\n",
    "grid.fit(X_vect,y)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.91949, std: 0.01253, params: {'colsample_bytree': 0.2},\n",
       "  mean: 0.92220, std: 0.01029, params: {'colsample_bytree': 0.3},\n",
       "  mean: 0.92265, std: 0.01150, params: {'colsample_bytree': 0.4}],\n",
       " {'colsample_bytree': 0.4},\n",
       " 0.9226487759435736)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fine-tunning col_subsample_bytree\n",
    "param_test1 = {\n",
    " \n",
    "\n",
    " 'colsample_bytree':[i/10.0 for i in range(2,5)]\n",
    "}\n",
    "\n",
    "estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=61, max_depth=14,\n",
    " min_child_weight=13, gamma=20, subsample=0.9, colsample_bytree=0.6,\n",
    " objective= 'binary:logistic', nthread=-1, seed=27, scale_pos_weight = 1/np.mean(y))\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator, param_grid = param_test1, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=5 )\n",
    "\n",
    "grid.fit(X_vect,y)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Regularization Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.92314, std: 0.01102, params: {'reg_alpha': 1e-05},\n",
       "  mean: 0.92318, std: 0.01094, params: {'reg_alpha': 0.01},\n",
       "  mean: 0.92290, std: 0.01057, params: {'reg_alpha': 0.1},\n",
       "  mean: 0.92188, std: 0.01129, params: {'reg_alpha': 1},\n",
       "  mean: 0.89599, std: 0.01668, params: {'reg_alpha': 100}],\n",
       " {'reg_alpha': 0.01},\n",
       " 0.9231790171674564)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "}\n",
    "\n",
    "estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=61, max_depth=14,\n",
    " min_child_weight=13, gamma=20, subsample=0.9, colsample_bytree=0.4,\n",
    " objective= 'binary:logistic', nthread=-1, seed=27, scale_pos_weight = (1- np.mean(y))/np.mean(y))\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator, param_grid = param_test1, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=5 )\n",
    "\n",
    "grid.fit(X_vect,y)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retuning the number of trees with a reduced learning rate to reduce variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.819446+0.0178806\ttrain-error:0.245799+0.0706549\ttest-auc:0.767934+0.0224427\ttest-error:0.263798+0.0773849\n",
      "[1]\ttrain-auc:0.882387+0.0213404\ttrain-error:0.188934+0.0411122\ttest-auc:0.831802+0.0248598\ttest-error:0.21332+0.0432277\n",
      "[2]\ttrain-auc:0.901486+0.0208899\ttrain-error:0.179235+0.0476842\ttest-auc:0.850496+0.022933\ttest-error:0.200137+0.0459522\n",
      "[3]\ttrain-auc:0.909917+0.023141\ttrain-error:0.172131+0.0378193\ttest-auc:0.861715+0.0284715\ttest-error:0.192486+0.0372449\n",
      "[4]\ttrain-auc:0.924261+0.00867757\ttrain-error:0.160724+0.0300463\ttest-auc:0.88061+0.0106593\ttest-error:0.182855+0.0303133\n",
      "[5]\ttrain-auc:0.927617+0.00998393\ttrain-error:0.151161+0.0247658\ttest-auc:0.88586+0.0106756\ttest-error:0.172268+0.0258579\n",
      "[6]\ttrain-auc:0.930855+0.00848786\ttrain-error:0.14054+0.0209085\ttest-auc:0.891235+0.0121375\ttest-error:0.159631+0.015445\n",
      "[7]\ttrain-auc:0.933549+0.00766836\ttrain-error:0.139925+0.0285864\ttest-auc:0.894487+0.0129729\ttest-error:0.159768+0.0272248\n",
      "[8]\ttrain-auc:0.938664+0.0048037\ttrain-error:0.12985+0.0145608\ttest-auc:0.9007+0.00783542\ttest-error:0.149317+0.0123409\n",
      "[9]\ttrain-auc:0.941478+0.00300121\ttrain-error:0.128074+0.00922879\ttest-auc:0.903723+0.0055926\ttest-error:0.143647+0.00586065\n",
      "[10]\ttrain-auc:0.941928+0.00312631\ttrain-error:0.127083+0.0073327\ttest-auc:0.90525+0.00584356\ttest-error:0.144057+0.00422284\n",
      "[11]\ttrain-auc:0.943501+0.00326751\ttrain-error:0.126093+0.00543194\ttest-auc:0.907105+0.00458692\ttest-error:0.141393+0.00235997\n",
      "[12]\ttrain-auc:0.944212+0.00230688\ttrain-error:0.125376+0.00500003\ttest-auc:0.908099+0.00433435\ttest-error:0.139208+0.00439612\n",
      "[13]\ttrain-auc:0.945525+0.00229805\ttrain-error:0.122233+0.00516969\ttest-auc:0.90896+0.00469745\ttest-error:0.135519+0.00411354\n",
      "[14]\ttrain-auc:0.945721+0.00196821\ttrain-error:0.12015+0.00305566\ttest-auc:0.910508+0.00496232\ttest-error:0.134699+0.00519837\n",
      "[15]\ttrain-auc:0.946261+0.00201815\ttrain-error:0.121926+0.00259319\ttest-auc:0.910906+0.00458609\ttest-error:0.134631+0.00724502\n",
      "[16]\ttrain-auc:0.946889+0.00187875\ttrain-error:0.121482+0.00320983\ttest-auc:0.912028+0.00511941\ttest-error:0.134631+0.0055062\n",
      "[17]\ttrain-auc:0.947718+0.00171201\ttrain-error:0.11998+0.00302555\ttest-auc:0.911924+0.00495684\ttest-error:0.133607+0.006258\n",
      "[18]\ttrain-auc:0.948358+0.00131548\ttrain-error:0.12056+0.00300498\ttest-auc:0.911953+0.00401419\ttest-error:0.133743+0.00631442\n",
      "[19]\ttrain-auc:0.949223+0.00114405\ttrain-error:0.118784+0.00220128\ttest-auc:0.912551+0.0036155\ttest-error:0.132513+0.00816303\n",
      "[20]\ttrain-auc:0.949846+0.000698347\ttrain-error:0.118033+0.0029732\ttest-auc:0.913756+0.00406174\ttest-error:0.131148+0.00641053\n",
      "[21]\ttrain-auc:0.950426+0.000395748\ttrain-error:0.115198+0.00121315\ttest-auc:0.914731+0.00382411\ttest-error:0.12903+0.00664505\n",
      "[22]\ttrain-auc:0.951012+0.00080263\ttrain-error:0.114003+0.000854703\ttest-auc:0.915185+0.00387368\ttest-error:0.128347+0.00628778\n",
      "[23]\ttrain-auc:0.951531+0.000699137\ttrain-error:0.113456+0.00143032\ttest-auc:0.915445+0.00374507\ttest-error:0.127937+0.00645045\n",
      "[24]\ttrain-auc:0.951959+0.000897983\ttrain-error:0.112807+0.000931576\ttest-auc:0.916036+0.00382226\ttest-error:0.127254+0.00694541\n",
      "[25]\ttrain-auc:0.952204+0.000840183\ttrain-error:0.111783+0.0007296\ttest-auc:0.916399+0.00338834\ttest-error:0.128347+0.00519039\n",
      "[26]\ttrain-auc:0.95254+0.00143715\ttrain-error:0.112158+0.000696866\ttest-auc:0.91648+0.0033857\ttest-error:0.127937+0.00602407\n",
      "[27]\ttrain-auc:0.952985+0.00148897\ttrain-error:0.111885+0.000714864\ttest-auc:0.917241+0.00339407\ttest-error:0.127459+0.0068602\n",
      "[28]\ttrain-auc:0.953547+0.00152774\ttrain-error:0.111954+0.0012104\ttest-auc:0.917519+0.0033861\ttest-error:0.127391+0.00557539\n",
      "[29]\ttrain-auc:0.954027+0.00151218\ttrain-error:0.11192+0.00094027\ttest-auc:0.918164+0.00370503\ttest-error:0.127937+0.00502312\n",
      "[30]\ttrain-auc:0.954547+0.00113633\ttrain-error:0.110826+9.66379e-05\ttest-auc:0.918647+0.00367023\ttest-error:0.126366+0.00516875\n",
      "[31]\ttrain-auc:0.955086+0.00115049\ttrain-error:0.110143+0.00068499\ttest-auc:0.919078+0.00377474\ttest-error:0.126708+0.00568439\n",
      "[32]\ttrain-auc:0.955584+0.00112352\ttrain-error:0.110246+0.000653281\ttest-auc:0.919521+0.00407971\ttest-error:0.126639+0.00559167\n",
      "[33]\ttrain-auc:0.956165+0.000927155\ttrain-error:0.109563+0.000587826\ttest-auc:0.920035+0.00394821\ttest-error:0.125888+0.00601006\n",
      "[34]\ttrain-auc:0.95684+0.00129443\ttrain-error:0.108777+0.00138049\ttest-auc:0.920237+0.0041291\ttest-error:0.125546+0.00614814\n",
      "[35]\ttrain-auc:0.957323+0.00142816\ttrain-error:0.108504+0.00181182\ttest-auc:0.920476+0.00433006\ttest-error:0.124317+0.00584963\n",
      "[36]\ttrain-auc:0.957923+0.00144844\ttrain-error:0.108402+0.00238251\ttest-auc:0.920979+0.0042044\ttest-error:0.12336+0.00565158\n",
      "[37]\ttrain-auc:0.958566+0.00177194\ttrain-error:0.107889+0.00239405\ttest-auc:0.921296+0.00420439\ttest-error:0.123019+0.0053132\n",
      "[38]\ttrain-auc:0.959044+0.00186093\ttrain-error:0.107377+0.00266003\ttest-auc:0.921548+0.00400882\ttest-error:0.122746+0.00478828\n",
      "[39]\ttrain-auc:0.959239+0.00181589\ttrain-error:0.106933+0.00171481\ttest-auc:0.921884+0.00395746\ttest-error:0.122678+0.00528122\n",
      "[40]\ttrain-auc:0.959753+0.00167583\ttrain-error:0.106182+0.00159265\ttest-auc:0.922371+0.00382157\ttest-error:0.122336+0.0049519\n",
      "[41]\ttrain-auc:0.959998+0.0017115\ttrain-error:0.105191+0.00155687\ttest-auc:0.922476+0.00368788\ttest-error:0.122404+0.00518489\n",
      "[42]\ttrain-auc:0.96047+0.00181081\ttrain-error:0.104372+0.00145443\ttest-auc:0.922827+0.00371932\ttest-error:0.122882+0.00456778\n",
      "[43]\ttrain-auc:0.960984+0.00171624\ttrain-error:0.104167+0.00164011\ttest-auc:0.923331+0.00378472\ttest-error:0.122951+0.00449879\n",
      "[44]\ttrain-auc:0.961493+0.00186506\ttrain-error:0.103449+0.00143541\ttest-auc:0.923829+0.00362101\ttest-error:0.121517+0.00449256\n",
      "[45]\ttrain-auc:0.961721+0.00196074\ttrain-error:0.103142+0.00150872\ttest-auc:0.923948+0.00364476\ttest-error:0.121926+0.00451142\n",
      "[46]\ttrain-auc:0.962074+0.0019335\ttrain-error:0.102083+0.00198358\ttest-auc:0.924105+0.0036596\ttest-error:0.1222+0.00405134\n",
      "[47]\ttrain-auc:0.962137+0.00195132\ttrain-error:0.102015+0.00212684\ttest-auc:0.924055+0.00372085\ttest-error:0.122063+0.00436736\n",
      "[48]\ttrain-auc:0.962169+0.00196569\ttrain-error:0.102288+0.00211855\ttest-auc:0.924169+0.00363055\ttest-error:0.121721+0.00455466\n",
      "[49]\ttrain-auc:0.9625+0.00194865\ttrain-error:0.10222+0.00210056\ttest-auc:0.92428+0.00378354\ttest-error:0.121038+0.00404788\n",
      "[50]\ttrain-auc:0.962618+0.00183079\ttrain-error:0.102049+0.00229265\ttest-auc:0.924287+0.00376662\ttest-error:0.120287+0.00474393\n",
      "[51]\ttrain-auc:0.962912+0.00170521\ttrain-error:0.1014+0.0025465\ttest-auc:0.924309+0.00380592\ttest-error:0.120355+0.00513069\n",
      "[52]\ttrain-auc:0.963184+0.00173443\ttrain-error:0.10082+0.00278486\ttest-auc:0.924598+0.00372968\ttest-error:0.120082+0.00520287\n",
      "[53]\ttrain-auc:0.963198+0.00172795\ttrain-error:0.101161+0.00273686\ttest-auc:0.92462+0.00366532\ttest-error:0.120628+0.00485012\n",
      "[54]\ttrain-auc:0.963485+0.00169175\ttrain-error:0.100239+0.00206339\ttest-auc:0.924947+0.00363918\ttest-error:0.119331+0.00468001\n",
      "[55]\ttrain-auc:0.963634+0.00163606\ttrain-error:0.099795+0.00283292\ttest-auc:0.924965+0.00347723\ttest-error:0.118579+0.0045676\n",
      "[56]\ttrain-auc:0.963747+0.00158405\ttrain-error:0.099795+0.00301297\ttest-auc:0.925057+0.00352888\ttest-error:0.118238+0.00476466\n",
      "[57]\ttrain-auc:0.96397+0.00177378\ttrain-error:0.0994533+0.00259245\ttest-auc:0.925073+0.00361408\ttest-error:0.117896+0.00415385\n",
      "[58]\ttrain-auc:0.964157+0.00194293\ttrain-error:0.099522+0.00253257\ttest-auc:0.925271+0.00369028\ttest-error:0.118169+0.00420732\n",
      "[59]\ttrain-auc:0.964384+0.00168033\ttrain-error:0.0990437+0.00253906\ttest-auc:0.925113+0.00356834\ttest-error:0.118169+0.00436736\n",
      "[60]\ttrain-auc:0.96457+0.00173971\ttrain-error:0.098668+0.00255257\ttest-auc:0.925223+0.00362968\ttest-error:0.117896+0.00459501\n",
      "[61]\ttrain-auc:0.964808+0.00187364\ttrain-error:0.098429+0.00222363\ttest-auc:0.925221+0.00349791\ttest-error:0.11735+0.00468852\n",
      "[62]\ttrain-auc:0.964949+0.0019965\ttrain-error:0.0984633+0.00230162\ttest-auc:0.925133+0.00342854\ttest-error:0.116462+0.00464398\n",
      "[63]\ttrain-auc:0.965019+0.00193968\ttrain-error:0.0982583+0.00231113\ttest-auc:0.925254+0.00343976\ttest-error:0.116393+0.00394511\n",
      "[64]\ttrain-auc:0.965141+0.00182599\ttrain-error:0.0977117+0.00244844\ttest-auc:0.925335+0.00334205\ttest-error:0.115984+0.00378987\n",
      "[65]\ttrain-auc:0.965523+0.00186648\ttrain-error:0.0974383+0.00227408\ttest-auc:0.925437+0.00337102\ttest-error:0.115301+0.00380179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66]\ttrain-auc:0.965636+0.00186749\ttrain-error:0.097131+0.00175681\ttest-auc:0.925572+0.00328839\ttest-error:0.114891+0.00327125\n",
      "[67]\ttrain-auc:0.965872+0.00197403\ttrain-error:0.096892+0.00177521\ttest-auc:0.92569+0.00337467\ttest-error:0.114891+0.00293831\n",
      "[68]\ttrain-auc:0.966237+0.00191202\ttrain-error:0.0962773+0.00144\ttest-auc:0.925752+0.00343833\ttest-error:0.114481+0.00281159\n",
      "[69]\ttrain-auc:0.966464+0.00171551\ttrain-error:0.0963453+0.00123333\ttest-auc:0.925931+0.00356514\ttest-error:0.114345+0.00306691\n",
      "[70]\ttrain-auc:0.966583+0.00173617\ttrain-error:0.0963797+0.00133408\ttest-auc:0.926167+0.00369475\ttest-error:0.114686+0.00320204\n",
      "[71]\ttrain-auc:0.966673+0.00167199\ttrain-error:0.0958677+0.00178725\ttest-auc:0.926255+0.00373241\ttest-error:0.114481+0.00281159\n",
      "[72]\ttrain-auc:0.966683+0.00165998\ttrain-error:0.0959357+0.00181818\ttest-auc:0.926202+0.00369511\ttest-error:0.114276+0.00288045\n",
      "[73]\ttrain-auc:0.966732+0.00166305\ttrain-error:0.095936+0.0018974\ttest-auc:0.926301+0.00376843\ttest-error:0.114413+0.00275113\n",
      "[74]\ttrain-auc:0.966839+0.00177954\ttrain-error:0.0958333+0.00202573\ttest-auc:0.926323+0.00377263\ttest-error:0.114208+0.00262614\n",
      "[75]\ttrain-auc:0.967028+0.00155013\ttrain-error:0.0955943+0.0017084\ttest-auc:0.926405+0.00382705\ttest-error:0.114071+0.00282149\n",
      "[76]\ttrain-auc:0.967153+0.00144013\ttrain-error:0.0953213+0.00163559\ttest-auc:0.926506+0.00390005\ttest-error:0.114139+0.00278486\n",
      "[77]\ttrain-auc:0.96722+0.00130312\ttrain-error:0.0952187+0.00144231\ttest-auc:0.926484+0.00390663\ttest-error:0.114139+0.00278486\n",
      "[78]\ttrain-auc:0.967413+0.00131667\ttrain-error:0.095253+0.00158157\ttest-auc:0.926494+0.00387845\ttest-error:0.114344+0.00285441\n",
      "[79]\ttrain-auc:0.967575+0.00122607\ttrain-error:0.0951843+0.00154936\ttest-auc:0.926573+0.00395637\ttest-error:0.114208+0.00286091\n",
      "[80]\ttrain-auc:0.967716+0.00136819\ttrain-error:0.095082+0.00167136\ttest-auc:0.92665+0.00392014\ttest-error:0.114276+0.00282149\n",
      "[81]\ttrain-auc:0.967794+0.00131994\ttrain-error:0.094843+0.00147823\ttest-auc:0.92671+0.00396289\ttest-error:0.114139+0.00291746\n",
      "[82]\ttrain-auc:0.967861+0.00133766\ttrain-error:0.0948087+0.00153415\ttest-auc:0.926686+0.00401339\ttest-error:0.113935+0.00325734\n",
      "[83]\ttrain-auc:0.967825+0.00128957\ttrain-error:0.0948087+0.00153415\ttest-auc:0.926685+0.00401574\ttest-error:0.113866+0.00328872\n",
      "[84]\ttrain-auc:0.967825+0.00128957\ttrain-error:0.0947743+0.0015895\ttest-auc:0.926685+0.00401574\ttest-error:0.113798+0.00319367\n",
      "[85]\ttrain-auc:0.967927+0.00120062\ttrain-error:0.0945353+0.0016249\ttest-auc:0.926763+0.00398169\ttest-error:0.114413+0.00306876\n",
      "[86]\ttrain-auc:0.968047+0.00118924\ttrain-error:0.0945697+0.00180021\ttest-auc:0.926913+0.00414649\ttest-error:0.11414+0.00306691\n",
      "[87]\ttrain-auc:0.968105+0.00120498\ttrain-error:0.0943647+0.00171658\ttest-auc:0.926907+0.00415177\ttest-error:0.11414+0.00323143\n",
      "[88]\ttrain-auc:0.968265+0.00125292\ttrain-error:0.0941257+0.00185614\ttest-auc:0.926971+0.00426838\ttest-error:0.113798+0.00310924\n",
      "[89]\ttrain-auc:0.968346+0.00117357\ttrain-error:0.0940573+0.00197295\ttest-auc:0.927004+0.00428907\ttest-error:0.113661+0.00300388\n",
      "[90]\ttrain-auc:0.968391+0.00115637\ttrain-error:0.0938867+0.00167411\ttest-auc:0.927078+0.00435255\ttest-error:0.113593+0.00322855\n",
      "[91]\ttrain-auc:0.968391+0.00115637\ttrain-error:0.0938523+0.00167341\ttest-auc:0.927078+0.00435255\ttest-error:0.113525+0.0032958\n",
      "[92]\ttrain-auc:0.968466+0.00107434\ttrain-error:0.0934087+0.00190108\ttest-auc:0.92714+0.00441588\ttest-error:0.113251+0.003471\n",
      "[93]\ttrain-auc:0.968649+0.000953025\ttrain-error:0.0932033+0.00160142\ttest-auc:0.927204+0.00446996\ttest-error:0.112841+0.00360557\n",
      "[94]\ttrain-auc:0.968649+0.000953025\ttrain-error:0.0932377+0.00159629\ttest-auc:0.927204+0.00446996\ttest-error:0.112841+0.00360557\n",
      "[95]\ttrain-auc:0.968725+0.000947555\ttrain-error:0.0932373+0.00164577\ttest-auc:0.927348+0.00459361\ttest-error:0.1125+0.00389869\n",
      "[96]\ttrain-auc:0.968713+0.000932937\ttrain-error:0.093306+0.00148792\ttest-auc:0.927358+0.00457962\ttest-error:0.112432+0.00385303\n",
      "[97]\ttrain-auc:0.968735+0.000934771\ttrain-error:0.093511+0.00188265\ttest-auc:0.92744+0.00464045\ttest-error:0.11209+0.00406054\n",
      "[98]\ttrain-auc:0.968843+0.000905439\ttrain-error:0.0932373+0.0015895\ttest-auc:0.927558+0.00472966\ttest-error:0.112022+0.00446573\n",
      "[99]\ttrain-auc:0.968982+0.000865469\ttrain-error:0.093306+0.00160127\ttest-auc:0.927567+0.0047371\ttest-error:0.111817+0.00453418\n",
      "[100]\ttrain-auc:0.968981+0.000864803\ttrain-error:0.0932373+0.0017529\ttest-auc:0.927583+0.00471476\ttest-error:0.111748+0.00444687\n",
      "[101]\ttrain-auc:0.968991+0.00087484\ttrain-error:0.0931353+0.00183446\ttest-auc:0.927555+0.00475292\ttest-error:0.111749+0.00454339\n",
      "[102]\ttrain-auc:0.969063+0.000902699\ttrain-error:0.0932037+0.00188589\ttest-auc:0.927546+0.00472174\ttest-error:0.11168+0.00452697\n",
      "[103]\ttrain-auc:0.969229+0.000810235\ttrain-error:0.093306+0.00198402\ttest-auc:0.927559+0.00481508\ttest-error:0.11168+0.00443635\n",
      "[104]\ttrain-auc:0.969333+0.000718927\ttrain-error:0.092896+0.00163154\ttest-auc:0.927499+0.00477516\ttest-error:0.111475+0.00426906\n",
      "[105]\ttrain-auc:0.969451+0.000851078\ttrain-error:0.0927937+0.00192649\ttest-auc:0.92757+0.00467506\ttest-error:0.11168+0.00443635\n",
      "[106]\ttrain-auc:0.969519+0.000863531\ttrain-error:0.0929987+0.00190847\ttest-auc:0.92759+0.00469265\ttest-error:0.111544+0.00460121\n",
      "[107]\ttrain-auc:0.969627+0.000785894\ttrain-error:0.0925887+0.00196982\ttest-auc:0.927604+0.00470985\ttest-error:0.111612+0.00444056\n",
      "[108]\ttrain-auc:0.969712+0.000815137\ttrain-error:0.0924863+0.00200809\ttest-auc:0.927701+0.00479773\ttest-error:0.111612+0.00451831\n",
      "[109]\ttrain-auc:0.969712+0.000815137\ttrain-error:0.092486+0.00203602\ttest-auc:0.927701+0.00479773\ttest-error:0.111612+0.00451831\n",
      "[110]\ttrain-auc:0.969834+0.000897276\ttrain-error:0.092213+0.00219252\ttest-auc:0.927708+0.00483158\ttest-error:0.111475+0.00401553\n",
      "[111]\ttrain-auc:0.969899+0.000958094\ttrain-error:0.0920423+0.00253906\ttest-auc:0.927708+0.00483066\ttest-error:0.111407+0.00354683\n",
      "[112]\ttrain-auc:0.96997+0.00103039\ttrain-error:0.0920767+0.00274976\ttest-auc:0.927721+0.00481305\ttest-error:0.111202+0.0034871\n",
      "[113]\ttrain-auc:0.970067+0.00107281\ttrain-error:0.091974+0.00263818\ttest-auc:0.927729+0.00483201\ttest-error:0.110929+0.00385293\n",
      "[114]\ttrain-auc:0.970067+0.00107281\ttrain-error:0.09194+0.00248365\ttest-auc:0.927729+0.00483201\ttest-error:0.110724+0.00380173\n",
      "[115]\ttrain-auc:0.970169+0.000932272\ttrain-error:0.091496+0.00219438\ttest-auc:0.927791+0.00486162\ttest-error:0.110383+0.00368584\n",
      "[116]\ttrain-auc:0.970384+0.000940022\ttrain-error:0.091496+0.0023604\ttest-auc:0.927728+0.00489279\ttest-error:0.110588+0.00393214\n",
      "[117]\ttrain-auc:0.970376+0.00094916\ttrain-error:0.091496+0.0023604\ttest-auc:0.927739+0.00489806\ttest-error:0.110929+0.00396754\n",
      "[118]\ttrain-auc:0.970376+0.00094916\ttrain-error:0.0915983+0.00232883\ttest-auc:0.927739+0.00489806\ttest-error:0.110997+0.00404444\n",
      "[119]\ttrain-auc:0.970528+0.000865838\ttrain-error:0.0914277+0.00237836\ttest-auc:0.927718+0.00489822\ttest-error:0.111066+0.00385902\n",
      "[120]\ttrain-auc:0.970577+0.000882128\ttrain-error:0.0913933+0.00239114\ttest-auc:0.927709+0.00489018\ttest-error:0.110929+0.00403405\n",
      "[121]\ttrain-auc:0.970605+0.000893182\ttrain-error:0.0912227+0.0024566\ttest-auc:0.927728+0.00490783\ttest-error:0.110519+0.00456797\n",
      "[122]\ttrain-auc:0.970649+0.000914016\ttrain-error:0.0911887+0.00255276\ttest-auc:0.927792+0.00496758\ttest-error:0.110451+0.00476502\n",
      "[123]\ttrain-auc:0.970649+0.000914016\ttrain-error:0.0912227+0.00260053\ttest-auc:0.927792+0.00496758\ttest-error:0.110451+0.00476502\n",
      "[124]\ttrain-auc:0.97071+0.000949345\ttrain-error:0.0912227+0.00260053\ttest-auc:0.927816+0.00499034\ttest-error:0.110587+0.00469197\n"
     ]
    }
   ],
   "source": [
    "early_stopping = 10\n",
    "params = {'eta': 0.1, 'max_depth': 14, 'subsample': 0.9, 'colsample_bytree': 0.4, \n",
    "          'objective': 'binary:logistic', 'seed': 12345, 'silent': 1, 'eval_metric':['error', 'auc'], 'nthread':1,\n",
    "         'scale_pos_weight': 1/np.mean(y), 'reg_alpha':0.1, 'gamma': 20}\n",
    "\n",
    "xg_train = xgb.DMatrix(X_vect, label=y)\n",
    "cv = xgb.cv(params, xg_train, 5000, folds=5, early_stopping_rounds=early_stopping, verbose_eval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the element misclassified by our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.4, gamma=20, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=14, min_child_weight=13, missing=None, n_estimators=109,\n",
       "       n_jobs=1, nthread=1, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=8.792792792792792,\n",
       "       seed=27, silent=True, subsample=0.9)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = xgb.XGBClassifier( learning_rate =0.1, n_estimators=109, max_depth=14,\n",
    " min_child_weight=13, gamma=20, subsample=0.9, colsample_bytree=0.4,\n",
    " objective= 'binary:logistic', nthread=1, seed=27, scale_pos_weight = 1/np.mean(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vect,y, stratify=y)\n",
    "estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "y_hat = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 344,   72],\n",
       "       [ 355, 2889]], dtype=int64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Confusion Matrix\n",
    "confusion_matrix(y_test, y_hat, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : @united gates were DEFINITELY NOT FULL. We were parked and wheels were chocked. Customer service called it an \"UNMET ARRIVAL\"!!\n",
      "1 : @USAirways I will send an email with details Late Flightr. Thank you for responding.\n",
      "2 : @JetBlue 117 days maybe.\n",
      "3 : @USAirways US 728/Feb 21. Ground power shorts again for the third time. Weary German passenger deplanes. Makes me jealous. Auf wiedersehen!\n",
      "4 : @VirginAmerica spending my birthday night with you, DAL-DCA. Get me home!\n",
      "5 : @JetBlue flight 348 is a freaking nightmare tonight #sittingonthetarmac #delay\n",
      "6 : @USAirways pilot forgets to show up to work and Ricky J (gate agent) gives customers attitude. #usairwaysfail\n",
      "7 : @USAirways - you kind of screwed my day up, at least it was outbound, but you did save it by stranding me in a @RenHotels @MarriottRewards\n",
      "8 : @AmericanAir between your airline and @united I've now spent three extra days traveling (DOMESTIC) and spent hundreds of dollars...\n",
      "9 : @SouthwestAir you're killing me!! always #delayed from #SanDiego to #SF!! #ugh\n",
      "10 : @united has happened on other airlines and they've given out flight vouchers and been more transparent about the problems.\n",
      "11 : @united obviously no one knows a darn thing around here.  What are we to do if this does not get resolved? http://t.co/Ph8QJzaPKx\n",
      "12 : @USAirways can't seem to find the pilots for our 6pm flight, never seen anything like it before #drunkpilots http://t.co/Zira2z3UDc\n",
      "13 : @SouthwestAir yall have me sleeping in the airport until 4pm tomorrow! thanks\n",
      "14 : @USAirways the flights weren't from this morning... It was 4 flights 2 on Thursday and 2 on Monday which turned into Tuesday\n",
      "15 : @USAirways seriously buy some WD40 for A319 operating flight 634 from GEG to Phoenix. Every seat squeaks w every shift. Still on ground!\n",
      "16 : @united is screwing with my flights again at CAE? Why do you bother having flights here if they never leave on time or get Cancelled Flighted?\n",
      "17 : @USAirways - have dinner reservations in a few minutes -- and I've been trying to get through to you since BREAKFAST. #onholdwith\n",
      "18 : @SouthwestAir @poisonpill76 im in same boat.. 8am flight to Jacksonville... Get me out of here!!\n",
      "19 : @AmericanAir what's the status of flight 3494 XNA to DFW. I'm getting different reports?\n",
      "20 : @USAirways on flight # 556 delayed#connection help for flight # 561 # help via email\n",
      "21 : @USAirways have you heard about the debacle at DCA? What is happening with our New Orleans flight? #wewanttoknow\n",
      "22 : @united what's the hold up with flight 6475 from SLC to DEN??\n",
      "23 : @united of the airplane otherwise they would call the Police, because the flight had to take off before 12h30am\n",
      "24 : @jetblue is giving me false hope of ever getting home\n",
      "25 : @USAirways right, can you use a shuttle ticket from LGA to BOS for any shuttle flight? Probably gonna miss the 4pm.\n",
      "26 : @SouthwestAir where was the inclement weather? Other flights left DCA today and this plane appeared to be coming from AUS.\n",
      "27 : @united Why have you never held a plane for me? #HourDelay #MultipleDoorOpeningAndClosing #DangerOfGettingSnowedIn\n",
      "28 : @JetBlue What is going on with the flight from Buffalo to JFK? Have they figured anything out about the temperature and the tower yet?\n",
      "29 : @united shes been rescheduled for today, but with the frigid cold, being a possible reason for \"maintenance\" it could be the same issue 2day\n",
      "30 : @USAirways holding your passengers hostage during your computer crash is not customer friendly, let us off the plane or get us moving\n",
      "31 : @united Today was not your finest. All could have been prevented by one gate agent advising 200 passengers.\n",
      "32 : @USAirways look @Delta has made the right call on CHO. Please evaluate. http://t.co/3KhGjoFgpX\n",
      "33 : @united passengers should have been rerouted to match the intended arrival time (even if you have to incur cost to put us in other airlines)\n",
      "34 : @AmericanAir None of the #LAX flights into #DFW have been Cancelled Flightled. Those landing before and after ours are fine. Completely arbitrary.\n",
      "35 : @AmericanAir it's always better to find out equipment is INOP on the ground than in the air! #safetyfirst\n",
      "36 : @USAirways don't worry. Used own initiative and arriving much earlier than you planned for me\n",
      "37 : @AmericanAir Was put on hold for 5.5 hrs then got a call back at 11:20 pm, only to wait on hold for another hour.\n",
      "38 : @united flight 433 lets go you look like clowns no one is giving up 1st class for $500\n",
      "39 : @JetBlue honestly Im glad you didnt Cancelled Flight the flight, just that Ill be driving home at 3-4 am now. :/\n",
      "40 : @united well that's big of you but I don't have terribly high expectations at this point.\n",
      "41 : @AmericanAir sure is. What's more frustrating is being stranded at the airport with our fate in your hands.\n",
      "42 : @SouthwestAir hasn't evennotified us that theflight isdelayed via email/text/phone call.If we wererunning Late Flight I would be pissed #unreliable\n",
      "43 : @united I am a comedian and I promise I will make fun of you on stage tonight.\n",
      "44 : @united if you're going to Cancelled Flight flights today, do everyone a favor and get on it now. Don't stall the inevitable.\n",
      "45 : @USAirways freaking out about the fact you are fixing the engine from Charlotte to Orlando. #longday\n",
      "46 : @JetBlue flight 16 now sent back to the gate even though the app says its leaving. Anyone have a clue? It's 36 and sunny at JFK\n",
      "47 : @united possibly the worst airlineGave them three chances Second time in 2 weeks a flight has been delayed and or Cancelled Flightled due to mechanics\n",
      "48 : @USAirways on hold 2.5 hrs trying to reschedule our flight. Can anyone there please help us?\n",
      "49 : @United come on, reopen 1285 at ORD and clear your growing DC backlog\n",
      "50 : Made it to #Costa #Rica and back @JetBlue Missing it already!  #Pura #Vida!\n",
      "51 : @united If someone misses his connecting flight because of the airline, and it was the last flight of the day, how will you fix it?\n",
      "52 : @SouthwestAir but when do I get my gin &amp; tonic !?!?\n",
      "53 : @united they held the flight for our group of nearly 20 people.\n",
      "54 : @united just changed again.  This is crazy! #YourAgentsHaveNoClue http://t.co/A37N3oHOKL\n",
      "55 : @USAirways patience is what my connecting flight will need. Why do you guys do this to me every time I travel? Every. Single. Time.\n",
      "56 : @USAirways Pilot pleads to let boarding begin for full flight #1937 while waiting for 4th flight attendant. Denied by mission control. Grrrr\n",
      "57 : @united stay more than 24h traveling ans sleeping on the airports floor\n",
      "58 : @USAirways or how about power outlets at your seat if you're gonna keep us siting here forever?\n",
      "59 : @JetBlue there were seats but after waiting on hold for 20 minutes they were gone-now she has to stay another night in charleston-very pricy\n",
      "60 : @united From the air: Another missed cnxn 2day. ATC went on strike in Belize this AM. Now family &amp; I miss cnxn in EWR. Please see my DMs.\n",
      "61 : @united I'm counting on you, please don't let me down! #delayed #again #needtocatchmynextflight #alreadyrebookedonce\n",
      "62 : @united 1k and had problem getting out of FLL to IAH  sent DM to you about making my connection  please let me know\n",
      "63 : @AmericanAir aftr 10 hrs bng held hstg at mia bc aa refsd to get med bags. bag fnd aa then refsd to fix tkts they cnceld said $360 to fly\n",
      "64 : @JetBlue don't just cling on to the safety card. It safety was really an issue, then flight would have landed immediately. .\n",
      "65 : @JetBlue If it was simply a minor mechanical, why would they say that?\n",
      "66 : @USAirways I sympathize re:weather issues, but when times get tough, call in more folks. Over two hours on hold for 4-minute fix. #FAIL\n",
      "67 : @JetBlue even @Citi responded quicker via Twitter when they #fail.... Still on hold, hoping no one books the flight..... @JetBlue #fail\n",
      "68 : @SouthwestAir By the way the flight number was 1703, please feel free to fact check my complaints about leave time &amp; baggage time.\n",
      "69 : @USAirways yes thanks I have never seen anything like this and I come to Yuma all the time, big disappointment. There are 40+ still waiting\n",
      "70 : @AmericanAir aa223 being ignored\n",
      "71 : @AmericanAir: @Dumas2TTG Good morning, Tamara. We'll try to get you comfortably on a flight as soon as we can.#NoXTraLegRoom #NoCoatCloset\n"
     ]
    }
   ],
   "source": [
    "comp = (y_hat != y_test) & (y_test == 1)\n",
    "idx = comp[comp == True].index\n",
    "misclassified = list(X.iloc[idx])\n",
    "for i, item in enumerate(misclassified):\n",
    "    print('{} : {}'.format(i, item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : toperson0 hey what s happen with hashtag0 zurich city0 appears to have squawk 7700 and land at london heathrow\n",
      "1 : toperson0 there be seat but after wait on hold for delay0 they be go now she have to stay another night in charleston very pricy\n",
      "2 : toperson0 117 day maybe\n",
      "3 : toperson0 seriously buy some wd40 for a319 operate flight 634 from city0 to phoenix every seat squeak w every shift still on ground\n",
      "4 : toperson0 finally about to take off thanks for the customer service by the g8 rep c17 city0 any idea what cause the system failure\n",
      "5 : toperson0 the most frustrating fly experience one continuous i dunno what time you will leave today dunno that s code for\n",
      "6 : toperson0 can t get back lose time\n",
      "7 : toperson0 finally get through after delay0 and all set thanks\n",
      "8 : toperson0 stop picking on u city0 link0\n",
      "9 : toperson0 you stand me up last night but i m give you a second chance i m just a boy tweet an airline ask them to fly him home\n",
      "10 : toperson0 even toperson0 respond quicker via twitter when they hashtag0 still on hold hop no one book the flight toperson0 hashtag0\n",
      "11 : toperson0 mechanical failure isn t boston fault\n",
      "12 : toperson0 yall have me sleep in the airport until 4pm tomorrow thanks\n",
      "13 : toperson0 aftr delay0 bng hold hstg at mia bc aa refsd to get med bag bag fnd aa then refsd to fix tkts they cnceld say 360 to fly\n",
      "14 : we didn t need this city0 toperson0 our fleet s on fleek link0\n",
      "15 : toperson0 freak out about the fact you be fix the engine from charlotte to orlando hashtag0\n",
      "16 : toperson0 they hold the flight for our group of nearly 20 people\n",
      "17 : toperson0 between your airline and toperson0 i ve now spent three extra day travel domestic and spent hundred of dollar\n",
      "18 : toperson0 why win t you let me leave newark\n",
      "19 : toperson0 aa45 city0 to city0\n",
      "20 : toperson0 i can t wait for the toperson0 and toperson0 to find out how poorly toperson0 handle this situation hashtag0 link0\n",
      "21 : toperson0 why be my time less important to you hashtag0 hashtag0 hashtag0\n",
      "22 : toperson0 that s gotta be a new record delay0 in the air delay0 wait and 0 bag deliver to destination\n",
      "23 : toperson0 i appreciate the response but the constant changing of the flight time be frustrate to say the least hashtag0\n",
      "24 : toperson0 it s ok i forgive you guy\n",
      "25 : toperson0 look at all the delta flight that land while your pilot claim they couldn t fly 6232 to city0 link0\n",
      "26 : toperson0 toperson0 i be suppose to get from fresno to pittsburgh by 10pm instead i m now not get in until 4pm tomorrow\n",
      "27 : toperson0 ok i have that pretty sure i have it before too but will wait and see what happen\n",
      "28 : toperson0 strand a student try to get to birmingham in city0 overnight w o any help you can do good than that toperson0\n",
      "29 : toperson0 i always brag about ur service but very disappointed today apparently i ll be sleep on floor of dallas airport tonight \n",
      "30 : toperson0 dont get me wrong i love fly with you tv free bag and descent fare the recent plane problem be just annoy\n",
      "31 : toperson0 i appreciate that when i fly with you in january the check engine light come on right before take off now it be a computer\n",
      "32 : toperson0 so apparently all of your computer crash at city0 any update on when flight will be move\n",
      "33 : toperson0 toperson0 birthday be the 24th and he s not see imagine dragon at hashtag0\n",
      "34 : toperson0 flight 433 let go you look like clown no one be give up 1st class for 500\n",
      "35 : toperson0 what be wrong in boston why be only your plain not leave tonight son need to get back to state college city0\n",
      "36 : toperson0 why 730 and not 645 a schedule\n",
      "37 : toperson0 city0 she s on her way now but think id detail the extravaganza for you hashtag0 hashtag0 link0\n",
      "38 : toperson0 and wait\n",
      "39 : toperson0 right can you use a shuttle ticket from city0 to city0 for any shuttle flight probably gonna miss the 4pm\n",
      "40 : toperson0 delay0 hold be delay0 delay0 toperson0\n",
      "41 : toperson0 passenger should have be rerouted to match the intended arrival time even if you have to incur cost to put u in other airline\n",
      "42 : toperson0 my friend from boston stuck in denver her name jane toperson0 please contact her\n",
      "43 : toperson0 yeah try harder no cater truck in sight can we just leave now w o the food win t eat anyway\n",
      "44 : toperson0 thanks for the response i know it s not your fault but im in city0 in t5 and hungry if you want to stop by      \n",
      "45 : toperson0 get flight 16 into the air\n",
      "46 : toperson0 all day\n",
      "47 : toperson0 city0 me and i can explain the whole story i should have be on an early flight not us1937\n",
      "48 : toperson0 i will send an email with detail late flightr thank you for respond\n",
      "49 : toperson0 not cancel flightling hashtag0 because you don t want to put u in a hotel for night be our guess\n",
      "50 : toperson0 guess that s what i get for brag on you i fly you by choice unlike others who fly because they don t have other option city0\n",
      "51 : toperson0 standby so far for two of u with snow storm hit iowa tomorrow we could be stick here for two day\n",
      "52 : toperson0 flight 317\n",
      "53 : toperson0 need2know if ill be able to reach city0 today rather stay in city0 that fly to city0 amp find out i can t travel link0\n",
      "54 : toperson0 but seriously if my cat dead i m go to be piss very thankful because it be evil and want my soul\n",
      "55 : toperson0 surely there be some other way to help me i can t really afford another delay0 now pls follow me so we can city0\n",
      "56 : toperson0 would it kill you to let me know how many minute i might be on hold\n",
      "57 : make it to hashtag0 hashtag0 and back toperson0 miss it already hashtag0 hashtag0\n",
      "58 : toperson0 stuck in city0 because united can t find their airplane lol hashtag0 terminal right now\n",
      "59 : toperson0 what s the status of flight 3494 city0 to city0 i m get different report\n",
      "60 : toperson0 will you fly me to somewhere warm i m tire of this snow\n",
      "61 : toperson0 yes thank i have never see anything like this and i come to yuma all the time big disappointment there be 40 still wait\n",
      "62 : toperson0 toperson0 toperson0 75yo mom say hire more staff to accommodate your audience you could learn lot from her she s off to city0\n",
      "63 : toperson0 don t just cling on to the safety card it safety be really an issue then flight would have land immediately\n",
      "64 : toperson0 be screw with my flight again at city0 why do you bother have flight here if they never leave on time or get cancel flighted\n",
      "65 : toperson0 depart from where it depart city0 and be due to land at city0 around 8pm\n",
      "66 : toperson0 break my heart at city0 delay0 toperson0\n",
      "67 : toperson0 you be lightyears ahead of the security control at newark airport they waste 40 precious min w bad efficiency hashtag0\n",
      "68 : toperson0 anything serious i should worry about\n",
      "69 : toperson0 typical can response from city0 fix your crappy performing last place ranked airline in this century\n",
      "70 : keep waiting time do not cost anything for toperson0 hundred people how payed thousand dollar with no feedback link0\n"
     ]
    }
   ],
   "source": [
    "processed_misclassified = process.transform(misclassified)\n",
    "for i, item in enumerate(processed_misclassified):\n",
    "    print('{} : {}'.format(i, item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Saving the final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('processor', NLTKPreprocessor(lower=True,\n",
       "         punct={'/', '=', '|', '{', '*', \"'\", '`', '-', '\\\\', ',', '@', '}', '>', '[', '_', '$', '%', '+', ')', '#', '&', '~', '!', '?', ':', '^', '(', ';', '\"', '.', ']', '<'},\n",
       "         remove_stopwords=True,\n",
       "         stopwords={\"you're\", 'between',...a=0, reg_lambda=1, scale_pos_weight=8.792792792792792,\n",
       "       seed=27, silent=True, subsample=0.9))])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor = NLTKPreprocessor()\n",
    "vect = CountVectorizer()\n",
    "boost = xgb.XGBClassifier( learning_rate =0.1, n_estimators=109, max_depth=14,\n",
    " min_child_weight=13, gamma=20, subsample=0.9, colsample_bytree=0.4,\n",
    " objective= 'binary:logistic', nthread=1, seed=27, scale_pos_weight = 1/np.mean(y))\n",
    "pipe= Pipeline([('processor', processor), ('vect', vect) , ('boost', boost)])\n",
    "\n",
    "\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../saved_models/xgboost.pkl']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(pipe, '../saved_models/xgboost.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "I tune the penality term and the gamma for this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.91683, std: 0.00925, params: {'C': 10, 'gamma': 0.005606638259699484},\n",
       "  mean: 0.91273, std: 0.00906, params: {'C': 10, 'gamma': 0.011213276519398968},\n",
       "  mean: 0.90576, std: 0.00913, params: {'C': 10, 'gamma': 0.03363982955819691},\n",
       "  mean: 0.90643, std: 0.00994, params: {'C': 30, 'gamma': 0.005606638259699484},\n",
       "  mean: 0.89913, std: 0.01067, params: {'C': 30, 'gamma': 0.011213276519398968},\n",
       "  mean: 0.90089, std: 0.00987, params: {'C': 30, 'gamma': 0.03363982955819691},\n",
       "  mean: 0.89019, std: 0.01132, params: {'C': 100, 'gamma': 0.005606638259699484},\n",
       "  mean: 0.89087, std: 0.01072, params: {'C': 100, 'gamma': 0.011213276519398968},\n",
       "  mean: 0.89761, std: 0.01096, params: {'C': 100, 'gamma': 0.03363982955819691}],\n",
       " {'C': 10, 'gamma': 0.005606638259699484},\n",
       " 0.9168252645709293)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    " 'C': [10, 30, 100],\n",
    " 'gamma': [x/X_vect.shape[1] for x in [50, 100, 300]]\n",
    "}\n",
    "\n",
    "estimator = SVC(class_weight='balanced')\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator, param_grid = param_test1, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=5 )\n",
    "\n",
    "grid.fit(X_vect,y)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.92130, std: 0.00993, params: {'gamma': 0.0006727965911639381},\n",
       "  mean: 0.92111, std: 0.00952, params: {'gamma': 0.0008970621215519175},\n",
       "  mean: 0.92084, std: 0.00921, params: {'gamma': 0.001121327651939897}],\n",
       " {'gamma': 0.0006727965911639381},\n",
       " 0.9212965566722794)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test1 = {\n",
    " \n",
    " 'gamma': [x/X_vect.shape[1] for x in [6, 8, 10]]\n",
    "}\n",
    "\n",
    "estimator = SVC(C = 10, class_weight='balanced')\n",
    "\n",
    "grid = GridSearchCV(estimator=estimator, param_grid = param_test1, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=5 )\n",
    "\n",
    "grid.fit(X_vect, y)\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM\n",
    "The light GBM is another boost algorithm but it defers from the XGBoost in that tree are grown by leafs instead of by level (so you can get an imbalanced tree). For us this mainly means that the number of leafs because amore accurate way to control the complexity of the model since there is no straightforward mappingto max_depth anymore).\n",
    "Besides that, tuning the rest of the parameters is more or less the same game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's binary_logloss: 0.673076 + 0.00178873\tcv_agg's auc: 0.775327 + 0.0283684\n",
      "[2]\tcv_agg's binary_logloss: 0.648858 + 0.00274277\tcv_agg's auc: 0.842357 + 0.0196507\n",
      "[3]\tcv_agg's binary_logloss: 0.626243 + 0.00427477\tcv_agg's auc: 0.850015 + 0.0194766\n",
      "[4]\tcv_agg's binary_logloss: 0.607964 + 0.00597687\tcv_agg's auc: 0.854476 + 0.0200179\n",
      "[5]\tcv_agg's binary_logloss: 0.590809 + 0.00765991\tcv_agg's auc: 0.8557 + 0.0200911\n",
      "[6]\tcv_agg's binary_logloss: 0.577598 + 0.00895329\tcv_agg's auc: 0.857441 + 0.0203729\n",
      "[7]\tcv_agg's binary_logloss: 0.568579 + 0.00933776\tcv_agg's auc: 0.860863 + 0.0204203\n",
      "[8]\tcv_agg's binary_logloss: 0.557027 + 0.0105478\tcv_agg's auc: 0.861748 + 0.0209941\n",
      "[9]\tcv_agg's binary_logloss: 0.547401 + 0.011677\tcv_agg's auc: 0.86167 + 0.0205548\n",
      "[10]\tcv_agg's binary_logloss: 0.539468 + 0.012528\tcv_agg's auc: 0.862497 + 0.0204581\n",
      "[11]\tcv_agg's binary_logloss: 0.531782 + 0.0134918\tcv_agg's auc: 0.86328 + 0.0204828\n",
      "[12]\tcv_agg's binary_logloss: 0.526752 + 0.0141528\tcv_agg's auc: 0.865211 + 0.0210858\n",
      "[13]\tcv_agg's binary_logloss: 0.520202 + 0.0150903\tcv_agg's auc: 0.865652 + 0.0212245\n",
      "[14]\tcv_agg's binary_logloss: 0.514316 + 0.0157585\tcv_agg's auc: 0.865883 + 0.0212117\n",
      "[15]\tcv_agg's binary_logloss: 0.509917 + 0.0164668\tcv_agg's auc: 0.866874 + 0.021497\n",
      "[16]\tcv_agg's binary_logloss: 0.504623 + 0.01695\tcv_agg's auc: 0.867633 + 0.020784\n",
      "[17]\tcv_agg's binary_logloss: 0.499501 + 0.0178973\tcv_agg's auc: 0.867711 + 0.0210597\n",
      "[18]\tcv_agg's binary_logloss: 0.495373 + 0.0186445\tcv_agg's auc: 0.867591 + 0.0211599\n",
      "[19]\tcv_agg's binary_logloss: 0.492645 + 0.0187005\tcv_agg's auc: 0.868809 + 0.0211594\n",
      "[20]\tcv_agg's binary_logloss: 0.48927 + 0.0191779\tcv_agg's auc: 0.869248 + 0.0211237\n",
      "[21]\tcv_agg's binary_logloss: 0.486177 + 0.0198678\tcv_agg's auc: 0.869433 + 0.0212484\n",
      "[22]\tcv_agg's binary_logloss: 0.483538 + 0.0204397\tcv_agg's auc: 0.869796 + 0.021367\n",
      "[23]\tcv_agg's binary_logloss: 0.481188 + 0.0208124\tcv_agg's auc: 0.870087 + 0.0213167\n",
      "[24]\tcv_agg's binary_logloss: 0.478778 + 0.0210978\tcv_agg's auc: 0.87079 + 0.0210245\n",
      "[25]\tcv_agg's binary_logloss: 0.476502 + 0.0215589\tcv_agg's auc: 0.870802 + 0.0211605\n",
      "[26]\tcv_agg's binary_logloss: 0.473732 + 0.0212751\tcv_agg's auc: 0.873088 + 0.0209293\n",
      "[27]\tcv_agg's binary_logloss: 0.471712 + 0.0216836\tcv_agg's auc: 0.873136 + 0.0208254\n",
      "[28]\tcv_agg's binary_logloss: 0.469615 + 0.0216538\tcv_agg's auc: 0.874293 + 0.0210176\n",
      "[29]\tcv_agg's binary_logloss: 0.467844 + 0.0217859\tcv_agg's auc: 0.874658 + 0.0209142\n",
      "[30]\tcv_agg's binary_logloss: 0.465939 + 0.022082\tcv_agg's auc: 0.875063 + 0.0208058\n",
      "[31]\tcv_agg's binary_logloss: 0.464148 + 0.022086\tcv_agg's auc: 0.876026 + 0.0209342\n",
      "[32]\tcv_agg's binary_logloss: 0.462588 + 0.0224687\tcv_agg's auc: 0.876224 + 0.0209438\n",
      "[33]\tcv_agg's binary_logloss: 0.461414 + 0.0225375\tcv_agg's auc: 0.876522 + 0.0207857\n",
      "[34]\tcv_agg's binary_logloss: 0.460192 + 0.0227167\tcv_agg's auc: 0.877001 + 0.0207002\n",
      "[35]\tcv_agg's binary_logloss: 0.45908 + 0.0230522\tcv_agg's auc: 0.877088 + 0.0206129\n",
      "[36]\tcv_agg's binary_logloss: 0.458093 + 0.0233775\tcv_agg's auc: 0.877218 + 0.0206286\n",
      "[37]\tcv_agg's binary_logloss: 0.456468 + 0.0231539\tcv_agg's auc: 0.878221 + 0.0206973\n",
      "[38]\tcv_agg's binary_logloss: 0.455353 + 0.023503\tcv_agg's auc: 0.878233 + 0.0206218\n",
      "[39]\tcv_agg's binary_logloss: 0.453768 + 0.0233526\tcv_agg's auc: 0.879174 + 0.0205878\n",
      "[40]\tcv_agg's binary_logloss: 0.452971 + 0.023432\tcv_agg's auc: 0.879239 + 0.0205493\n",
      "[41]\tcv_agg's binary_logloss: 0.451751 + 0.0234086\tcv_agg's auc: 0.879905 + 0.0204767\n",
      "[42]\tcv_agg's binary_logloss: 0.451156 + 0.0235488\tcv_agg's auc: 0.879992 + 0.0204041\n",
      "[43]\tcv_agg's binary_logloss: 0.450681 + 0.0238063\tcv_agg's auc: 0.880032 + 0.0204889\n",
      "[44]\tcv_agg's binary_logloss: 0.449963 + 0.024018\tcv_agg's auc: 0.880193 + 0.0204855\n",
      "[45]\tcv_agg's binary_logloss: 0.449212 + 0.0241303\tcv_agg's auc: 0.88047 + 0.0204813\n",
      "[46]\tcv_agg's binary_logloss: 0.448597 + 0.0242102\tcv_agg's auc: 0.880677 + 0.0205008\n",
      "[47]\tcv_agg's binary_logloss: 0.447983 + 0.0242809\tcv_agg's auc: 0.88096 + 0.0207123\n",
      "[48]\tcv_agg's binary_logloss: 0.447306 + 0.0243775\tcv_agg's auc: 0.881085 + 0.0206691\n",
      "[49]\tcv_agg's binary_logloss: 0.446933 + 0.0244918\tcv_agg's auc: 0.881135 + 0.0207236\n",
      "[50]\tcv_agg's binary_logloss: 0.446387 + 0.0243658\tcv_agg's auc: 0.88145 + 0.0206341\n",
      "[51]\tcv_agg's binary_logloss: 0.445948 + 0.0245453\tcv_agg's auc: 0.88159 + 0.0206247\n",
      "[52]\tcv_agg's binary_logloss: 0.445555 + 0.0245517\tcv_agg's auc: 0.881709 + 0.0205446\n",
      "[53]\tcv_agg's binary_logloss: 0.445241 + 0.0245368\tcv_agg's auc: 0.881799 + 0.0204709\n",
      "[54]\tcv_agg's binary_logloss: 0.444823 + 0.0245451\tcv_agg's auc: 0.881946 + 0.0205716\n",
      "[55]\tcv_agg's binary_logloss: 0.444462 + 0.0245404\tcv_agg's auc: 0.882114 + 0.0205569\n",
      "[56]\tcv_agg's binary_logloss: 0.444181 + 0.0246707\tcv_agg's auc: 0.882153 + 0.0205349\n",
      "[57]\tcv_agg's binary_logloss: 0.443918 + 0.0247072\tcv_agg's auc: 0.882253 + 0.0205439\n",
      "[58]\tcv_agg's binary_logloss: 0.443734 + 0.0247178\tcv_agg's auc: 0.882341 + 0.0205145\n",
      "[59]\tcv_agg's binary_logloss: 0.443553 + 0.0247239\tcv_agg's auc: 0.882355 + 0.020551\n",
      "[60]\tcv_agg's binary_logloss: 0.44324 + 0.0248995\tcv_agg's auc: 0.882444 + 0.0205616\n",
      "[61]\tcv_agg's binary_logloss: 0.443027 + 0.0249223\tcv_agg's auc: 0.882567 + 0.0205855\n",
      "[62]\tcv_agg's binary_logloss: 0.442851 + 0.0250391\tcv_agg's auc: 0.882573 + 0.0205852\n",
      "[63]\tcv_agg's binary_logloss: 0.442746 + 0.025065\tcv_agg's auc: 0.882595 + 0.0205924\n",
      "[64]\tcv_agg's binary_logloss: 0.44258 + 0.025029\tcv_agg's auc: 0.882691 + 0.0205905\n",
      "[65]\tcv_agg's binary_logloss: 0.442476 + 0.0250522\tcv_agg's auc: 0.882726 + 0.0205907\n",
      "[66]\tcv_agg's binary_logloss: 0.442389 + 0.0250931\tcv_agg's auc: 0.882758 + 0.0206356\n",
      "[67]\tcv_agg's binary_logloss: 0.442286 + 0.0250937\tcv_agg's auc: 0.882811 + 0.0206291\n",
      "[68]\tcv_agg's binary_logloss: 0.442207 + 0.0250562\tcv_agg's auc: 0.882852 + 0.0206151\n",
      "[69]\tcv_agg's binary_logloss: 0.442158 + 0.0250554\tcv_agg's auc: 0.882871 + 0.0206061\n",
      "[70]\tcv_agg's binary_logloss: 0.442106 + 0.0250536\tcv_agg's auc: 0.88291 + 0.0206199\n",
      "[71]\tcv_agg's binary_logloss: 0.442104 + 0.0250541\tcv_agg's auc: 0.882908 + 0.0206194\n",
      "[72]\tcv_agg's binary_logloss: 0.442102 + 0.02505\tcv_agg's auc: 0.882907 + 0.0206216\n",
      "[73]\tcv_agg's binary_logloss: 0.442102 + 0.02505\tcv_agg's auc: 0.882907 + 0.0206216\n",
      "[74]\tcv_agg's binary_logloss: 0.442102 + 0.02505\tcv_agg's auc: 0.882907 + 0.0206216\n",
      "[75]\tcv_agg's binary_logloss: 0.442102 + 0.02505\tcv_agg's auc: 0.882907 + 0.0206216\n",
      "[76]\tcv_agg's binary_logloss: 0.442102 + 0.02505\tcv_agg's auc: 0.882907 + 0.0206216\n",
      "[77]\tcv_agg's binary_logloss: 0.442102 + 0.02505\tcv_agg's auc: 0.882907 + 0.0206216\n",
      "[78]\tcv_agg's binary_logloss: 0.442102 + 0.02505\tcv_agg's auc: 0.882907 + 0.0206216\n",
      "[79]\tcv_agg's binary_logloss: 0.442102 + 0.02505\tcv_agg's auc: 0.882907 + 0.0206216\n",
      "[80]\tcv_agg's binary_logloss: 0.442102 + 0.02505\tcv_agg's auc: 0.882907 + 0.0206216\n"
     ]
    }
   ],
   "source": [
    "#Tuning the number of trees for a given learning rate\n",
    "params = {'max_depth' : -1, 'objective': 'binary', 'num_leaves': 64, 'learning_rate': 0.1, 'max_bin': 512, \n",
    "          'subsample_for_bin': 200, 'subsample': 1, 'subsample_freq': 1, 'colsample_bytree': 0.8, 'reg_alpha': 5, \n",
    "          'reg_lambda': 10, 'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 5, 'class_weight': 'balanced',\n",
    "          'num_class' : 1, 'metric' : 'binary_error'}\n",
    "pos_weight = 1-np.mean(y)\n",
    "weight = y.apply(lambda x : pos_weight if x == 1 else 1 - pos_weight)\n",
    "dataset = lgb.Dataset(data=X_vect, label=y, weight=weight)\n",
    "cv = lgb.cv(params=params, train_set=dataset, num_boost_round=1000, nfold=10, stratified=True,\n",
    "                early_stopping_rounds=10, metrics=['binary', 'auc'], verbose_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.91231, std: 0.01938, params: {'min_data_in_leaf': 1, 'num_leaves': 50},\n",
       "  mean: 0.91298, std: 0.01963, params: {'min_data_in_leaf': 1, 'num_leaves': 70},\n",
       "  mean: 0.91339, std: 0.01964, params: {'min_data_in_leaf': 1, 'num_leaves': 96},\n",
       "  mean: 0.91270, std: 0.01983, params: {'min_data_in_leaf': 1, 'num_leaves': 110},\n",
       "  mean: 0.91225, std: 0.01969, params: {'min_data_in_leaf': 2, 'num_leaves': 50},\n",
       "  mean: 0.91316, std: 0.01982, params: {'min_data_in_leaf': 2, 'num_leaves': 70},\n",
       "  mean: 0.91356, std: 0.01930, params: {'min_data_in_leaf': 2, 'num_leaves': 96},\n",
       "  mean: 0.91307, std: 0.01991, params: {'min_data_in_leaf': 2, 'num_leaves': 110},\n",
       "  mean: 0.91238, std: 0.01947, params: {'min_data_in_leaf': 3, 'num_leaves': 50},\n",
       "  mean: 0.91338, std: 0.01949, params: {'min_data_in_leaf': 3, 'num_leaves': 70},\n",
       "  mean: 0.91304, std: 0.01935, params: {'min_data_in_leaf': 3, 'num_leaves': 96},\n",
       "  mean: 0.91331, std: 0.01939, params: {'min_data_in_leaf': 3, 'num_leaves': 110},\n",
       "  mean: 0.91218, std: 0.01962, params: {'min_data_in_leaf': 4, 'num_leaves': 50},\n",
       "  mean: 0.91323, std: 0.01952, params: {'min_data_in_leaf': 4, 'num_leaves': 70},\n",
       "  mean: 0.91291, std: 0.01980, params: {'min_data_in_leaf': 4, 'num_leaves': 96},\n",
       "  mean: 0.91300, std: 0.01939, params: {'min_data_in_leaf': 4, 'num_leaves': 110},\n",
       "  mean: 0.91212, std: 0.01980, params: {'min_data_in_leaf': 5, 'num_leaves': 50},\n",
       "  mean: 0.91276, std: 0.01918, params: {'min_data_in_leaf': 5, 'num_leaves': 70},\n",
       "  mean: 0.91299, std: 0.01933, params: {'min_data_in_leaf': 5, 'num_leaves': 96},\n",
       "  mean: 0.91303, std: 0.01941, params: {'min_data_in_leaf': 5, 'num_leaves': 110}],\n",
       " {'min_data_in_leaf': 2, 'num_leaves': 96},\n",
       " 0.9135565816465017)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tunning \n",
    "params = {'n_estimators':80, 'max_depth' : -1, 'objective': 'binary', 'num_leaves': 64, 'learning_rate': 0.05, \n",
    "          'max_bin': 512, 'subsample_for_bin': 200, 'subsample': 1, 'subsample_freq': 1, 'colsample_bytree': 0.8, \n",
    "          'reg_alpha': 5, 'reg_lambda': 10, 'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 5, \n",
    "          'class_weight': 'balanced', 'num_class' : 1, 'metric' : 'binary_error'}\n",
    "\n",
    "gridParams = {\n",
    "    'num_leaves': [50, 70, 96, 110],\n",
    "    'min_data_in_leaf' : [1, 2, 3, 4, 5],\n",
    "\n",
    "    }\n",
    "\n",
    "mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', \n",
    "          n_jobs = 5, # Updated from 'nthread' \n",
    "          silent = True,\n",
    "          **params)\n",
    "grid = GridSearchCV(estimator=mdl, param_grid = gridParams, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=10 )\n",
    "\n",
    "grid.fit(X_vect,y)#, categorical_feature=range(X_vect.shape[1]))\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning max_bin, subsampling and bagging parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.90779, std: 0.02203, params: {'colsample_bytree': 0.5, 'max_bin': 150, 'subsample': 0.5},\n",
       "  mean: 0.91075, std: 0.02123, params: {'colsample_bytree': 0.5, 'max_bin': 150, 'subsample': 0.6},\n",
       "  mean: 0.91178, std: 0.02085, params: {'colsample_bytree': 0.5, 'max_bin': 150, 'subsample': 0.7},\n",
       "  mean: 0.91238, std: 0.02120, params: {'colsample_bytree': 0.5, 'max_bin': 150, 'subsample': 0.8},\n",
       "  mean: 0.91248, std: 0.02045, params: {'colsample_bytree': 0.5, 'max_bin': 150, 'subsample': 0.9},\n",
       "  mean: 0.91302, std: 0.02031, params: {'colsample_bytree': 0.5, 'max_bin': 150, 'subsample': 1.0},\n",
       "  mean: 0.90779, std: 0.02203, params: {'colsample_bytree': 0.5, 'max_bin': 220, 'subsample': 0.5},\n",
       "  mean: 0.91075, std: 0.02123, params: {'colsample_bytree': 0.5, 'max_bin': 220, 'subsample': 0.6},\n",
       "  mean: 0.91178, std: 0.02085, params: {'colsample_bytree': 0.5, 'max_bin': 220, 'subsample': 0.7},\n",
       "  mean: 0.91238, std: 0.02120, params: {'colsample_bytree': 0.5, 'max_bin': 220, 'subsample': 0.8},\n",
       "  mean: 0.91248, std: 0.02045, params: {'colsample_bytree': 0.5, 'max_bin': 220, 'subsample': 0.9},\n",
       "  mean: 0.91302, std: 0.02031, params: {'colsample_bytree': 0.5, 'max_bin': 220, 'subsample': 1.0},\n",
       "  mean: 0.90779, std: 0.02203, params: {'colsample_bytree': 0.5, 'max_bin': 300, 'subsample': 0.5},\n",
       "  mean: 0.91075, std: 0.02123, params: {'colsample_bytree': 0.5, 'max_bin': 300, 'subsample': 0.6},\n",
       "  mean: 0.91178, std: 0.02085, params: {'colsample_bytree': 0.5, 'max_bin': 300, 'subsample': 0.7},\n",
       "  mean: 0.91238, std: 0.02120, params: {'colsample_bytree': 0.5, 'max_bin': 300, 'subsample': 0.8},\n",
       "  mean: 0.91248, std: 0.02045, params: {'colsample_bytree': 0.5, 'max_bin': 300, 'subsample': 0.9},\n",
       "  mean: 0.91302, std: 0.02031, params: {'colsample_bytree': 0.5, 'max_bin': 300, 'subsample': 1.0},\n",
       "  mean: 0.90779, std: 0.02203, params: {'colsample_bytree': 0.5, 'max_bin': 400, 'subsample': 0.5},\n",
       "  mean: 0.91075, std: 0.02123, params: {'colsample_bytree': 0.5, 'max_bin': 400, 'subsample': 0.6},\n",
       "  mean: 0.91178, std: 0.02085, params: {'colsample_bytree': 0.5, 'max_bin': 400, 'subsample': 0.7},\n",
       "  mean: 0.91238, std: 0.02120, params: {'colsample_bytree': 0.5, 'max_bin': 400, 'subsample': 0.8},\n",
       "  mean: 0.91248, std: 0.02045, params: {'colsample_bytree': 0.5, 'max_bin': 400, 'subsample': 0.9},\n",
       "  mean: 0.91302, std: 0.02031, params: {'colsample_bytree': 0.5, 'max_bin': 400, 'subsample': 1.0},\n",
       "  mean: 0.90779, std: 0.02203, params: {'colsample_bytree': 0.5, 'max_bin': 512, 'subsample': 0.5},\n",
       "  mean: 0.91075, std: 0.02123, params: {'colsample_bytree': 0.5, 'max_bin': 512, 'subsample': 0.6},\n",
       "  mean: 0.91178, std: 0.02085, params: {'colsample_bytree': 0.5, 'max_bin': 512, 'subsample': 0.7},\n",
       "  mean: 0.91238, std: 0.02120, params: {'colsample_bytree': 0.5, 'max_bin': 512, 'subsample': 0.8},\n",
       "  mean: 0.91248, std: 0.02045, params: {'colsample_bytree': 0.5, 'max_bin': 512, 'subsample': 0.9},\n",
       "  mean: 0.91302, std: 0.02031, params: {'colsample_bytree': 0.5, 'max_bin': 512, 'subsample': 1.0},\n",
       "  mean: 0.90864, std: 0.02280, params: {'colsample_bytree': 0.6, 'max_bin': 150, 'subsample': 0.5},\n",
       "  mean: 0.91069, std: 0.02118, params: {'colsample_bytree': 0.6, 'max_bin': 150, 'subsample': 0.6},\n",
       "  mean: 0.91127, std: 0.02117, params: {'colsample_bytree': 0.6, 'max_bin': 150, 'subsample': 0.7},\n",
       "  mean: 0.91234, std: 0.02032, params: {'colsample_bytree': 0.6, 'max_bin': 150, 'subsample': 0.8},\n",
       "  mean: 0.91263, std: 0.02032, params: {'colsample_bytree': 0.6, 'max_bin': 150, 'subsample': 0.9},\n",
       "  mean: 0.91342, std: 0.02013, params: {'colsample_bytree': 0.6, 'max_bin': 150, 'subsample': 1.0},\n",
       "  mean: 0.90864, std: 0.02280, params: {'colsample_bytree': 0.6, 'max_bin': 220, 'subsample': 0.5},\n",
       "  mean: 0.91069, std: 0.02118, params: {'colsample_bytree': 0.6, 'max_bin': 220, 'subsample': 0.6},\n",
       "  mean: 0.91127, std: 0.02117, params: {'colsample_bytree': 0.6, 'max_bin': 220, 'subsample': 0.7},\n",
       "  mean: 0.91234, std: 0.02032, params: {'colsample_bytree': 0.6, 'max_bin': 220, 'subsample': 0.8},\n",
       "  mean: 0.91263, std: 0.02032, params: {'colsample_bytree': 0.6, 'max_bin': 220, 'subsample': 0.9},\n",
       "  mean: 0.91342, std: 0.02013, params: {'colsample_bytree': 0.6, 'max_bin': 220, 'subsample': 1.0},\n",
       "  mean: 0.90864, std: 0.02280, params: {'colsample_bytree': 0.6, 'max_bin': 300, 'subsample': 0.5},\n",
       "  mean: 0.91069, std: 0.02118, params: {'colsample_bytree': 0.6, 'max_bin': 300, 'subsample': 0.6},\n",
       "  mean: 0.91127, std: 0.02117, params: {'colsample_bytree': 0.6, 'max_bin': 300, 'subsample': 0.7},\n",
       "  mean: 0.91234, std: 0.02032, params: {'colsample_bytree': 0.6, 'max_bin': 300, 'subsample': 0.8},\n",
       "  mean: 0.91263, std: 0.02032, params: {'colsample_bytree': 0.6, 'max_bin': 300, 'subsample': 0.9},\n",
       "  mean: 0.91342, std: 0.02013, params: {'colsample_bytree': 0.6, 'max_bin': 300, 'subsample': 1.0},\n",
       "  mean: 0.90864, std: 0.02280, params: {'colsample_bytree': 0.6, 'max_bin': 400, 'subsample': 0.5},\n",
       "  mean: 0.91069, std: 0.02118, params: {'colsample_bytree': 0.6, 'max_bin': 400, 'subsample': 0.6},\n",
       "  mean: 0.91127, std: 0.02117, params: {'colsample_bytree': 0.6, 'max_bin': 400, 'subsample': 0.7},\n",
       "  mean: 0.91234, std: 0.02032, params: {'colsample_bytree': 0.6, 'max_bin': 400, 'subsample': 0.8},\n",
       "  mean: 0.91263, std: 0.02032, params: {'colsample_bytree': 0.6, 'max_bin': 400, 'subsample': 0.9},\n",
       "  mean: 0.91342, std: 0.02013, params: {'colsample_bytree': 0.6, 'max_bin': 400, 'subsample': 1.0},\n",
       "  mean: 0.90864, std: 0.02280, params: {'colsample_bytree': 0.6, 'max_bin': 512, 'subsample': 0.5},\n",
       "  mean: 0.91069, std: 0.02118, params: {'colsample_bytree': 0.6, 'max_bin': 512, 'subsample': 0.6},\n",
       "  mean: 0.91127, std: 0.02117, params: {'colsample_bytree': 0.6, 'max_bin': 512, 'subsample': 0.7},\n",
       "  mean: 0.91234, std: 0.02032, params: {'colsample_bytree': 0.6, 'max_bin': 512, 'subsample': 0.8},\n",
       "  mean: 0.91263, std: 0.02032, params: {'colsample_bytree': 0.6, 'max_bin': 512, 'subsample': 0.9},\n",
       "  mean: 0.91342, std: 0.02013, params: {'colsample_bytree': 0.6, 'max_bin': 512, 'subsample': 1.0},\n",
       "  mean: 0.90907, std: 0.02169, params: {'colsample_bytree': 0.7, 'max_bin': 150, 'subsample': 0.5},\n",
       "  mean: 0.91122, std: 0.02061, params: {'colsample_bytree': 0.7, 'max_bin': 150, 'subsample': 0.6},\n",
       "  mean: 0.91201, std: 0.02060, params: {'colsample_bytree': 0.7, 'max_bin': 150, 'subsample': 0.7},\n",
       "  mean: 0.91237, std: 0.02026, params: {'colsample_bytree': 0.7, 'max_bin': 150, 'subsample': 0.8},\n",
       "  mean: 0.91260, std: 0.02025, params: {'colsample_bytree': 0.7, 'max_bin': 150, 'subsample': 0.9},\n",
       "  mean: 0.91323, std: 0.01994, params: {'colsample_bytree': 0.7, 'max_bin': 150, 'subsample': 1.0},\n",
       "  mean: 0.90907, std: 0.02169, params: {'colsample_bytree': 0.7, 'max_bin': 220, 'subsample': 0.5},\n",
       "  mean: 0.91122, std: 0.02061, params: {'colsample_bytree': 0.7, 'max_bin': 220, 'subsample': 0.6},\n",
       "  mean: 0.91201, std: 0.02060, params: {'colsample_bytree': 0.7, 'max_bin': 220, 'subsample': 0.7},\n",
       "  mean: 0.91237, std: 0.02026, params: {'colsample_bytree': 0.7, 'max_bin': 220, 'subsample': 0.8},\n",
       "  mean: 0.91260, std: 0.02025, params: {'colsample_bytree': 0.7, 'max_bin': 220, 'subsample': 0.9},\n",
       "  mean: 0.91323, std: 0.01994, params: {'colsample_bytree': 0.7, 'max_bin': 220, 'subsample': 1.0},\n",
       "  mean: 0.90907, std: 0.02169, params: {'colsample_bytree': 0.7, 'max_bin': 300, 'subsample': 0.5},\n",
       "  mean: 0.91122, std: 0.02061, params: {'colsample_bytree': 0.7, 'max_bin': 300, 'subsample': 0.6},\n",
       "  mean: 0.91201, std: 0.02060, params: {'colsample_bytree': 0.7, 'max_bin': 300, 'subsample': 0.7},\n",
       "  mean: 0.91237, std: 0.02026, params: {'colsample_bytree': 0.7, 'max_bin': 300, 'subsample': 0.8},\n",
       "  mean: 0.91260, std: 0.02025, params: {'colsample_bytree': 0.7, 'max_bin': 300, 'subsample': 0.9},\n",
       "  mean: 0.91323, std: 0.01994, params: {'colsample_bytree': 0.7, 'max_bin': 300, 'subsample': 1.0},\n",
       "  mean: 0.90907, std: 0.02169, params: {'colsample_bytree': 0.7, 'max_bin': 400, 'subsample': 0.5},\n",
       "  mean: 0.91122, std: 0.02061, params: {'colsample_bytree': 0.7, 'max_bin': 400, 'subsample': 0.6},\n",
       "  mean: 0.91201, std: 0.02060, params: {'colsample_bytree': 0.7, 'max_bin': 400, 'subsample': 0.7},\n",
       "  mean: 0.91237, std: 0.02026, params: {'colsample_bytree': 0.7, 'max_bin': 400, 'subsample': 0.8},\n",
       "  mean: 0.91260, std: 0.02025, params: {'colsample_bytree': 0.7, 'max_bin': 400, 'subsample': 0.9},\n",
       "  mean: 0.91323, std: 0.01994, params: {'colsample_bytree': 0.7, 'max_bin': 400, 'subsample': 1.0},\n",
       "  mean: 0.90907, std: 0.02169, params: {'colsample_bytree': 0.7, 'max_bin': 512, 'subsample': 0.5},\n",
       "  mean: 0.91122, std: 0.02061, params: {'colsample_bytree': 0.7, 'max_bin': 512, 'subsample': 0.6},\n",
       "  mean: 0.91201, std: 0.02060, params: {'colsample_bytree': 0.7, 'max_bin': 512, 'subsample': 0.7},\n",
       "  mean: 0.91237, std: 0.02026, params: {'colsample_bytree': 0.7, 'max_bin': 512, 'subsample': 0.8},\n",
       "  mean: 0.91260, std: 0.02025, params: {'colsample_bytree': 0.7, 'max_bin': 512, 'subsample': 0.9},\n",
       "  mean: 0.91323, std: 0.01994, params: {'colsample_bytree': 0.7, 'max_bin': 512, 'subsample': 1.0},\n",
       "  mean: 0.91004, std: 0.02102, params: {'colsample_bytree': 0.8, 'max_bin': 150, 'subsample': 0.5},\n",
       "  mean: 0.91160, std: 0.02025, params: {'colsample_bytree': 0.8, 'max_bin': 150, 'subsample': 0.6},\n",
       "  mean: 0.91193, std: 0.02032, params: {'colsample_bytree': 0.8, 'max_bin': 150, 'subsample': 0.7},\n",
       "  mean: 0.91237, std: 0.01957, params: {'colsample_bytree': 0.8, 'max_bin': 150, 'subsample': 0.8},\n",
       "  mean: 0.91313, std: 0.01979, params: {'colsample_bytree': 0.8, 'max_bin': 150, 'subsample': 0.9},\n",
       "  mean: 0.91356, std: 0.01930, params: {'colsample_bytree': 0.8, 'max_bin': 150, 'subsample': 1.0},\n",
       "  mean: 0.91004, std: 0.02102, params: {'colsample_bytree': 0.8, 'max_bin': 220, 'subsample': 0.5},\n",
       "  mean: 0.91160, std: 0.02025, params: {'colsample_bytree': 0.8, 'max_bin': 220, 'subsample': 0.6},\n",
       "  mean: 0.91193, std: 0.02032, params: {'colsample_bytree': 0.8, 'max_bin': 220, 'subsample': 0.7},\n",
       "  mean: 0.91237, std: 0.01957, params: {'colsample_bytree': 0.8, 'max_bin': 220, 'subsample': 0.8},\n",
       "  mean: 0.91313, std: 0.01979, params: {'colsample_bytree': 0.8, 'max_bin': 220, 'subsample': 0.9},\n",
       "  mean: 0.91356, std: 0.01930, params: {'colsample_bytree': 0.8, 'max_bin': 220, 'subsample': 1.0},\n",
       "  mean: 0.91004, std: 0.02102, params: {'colsample_bytree': 0.8, 'max_bin': 300, 'subsample': 0.5},\n",
       "  mean: 0.91160, std: 0.02025, params: {'colsample_bytree': 0.8, 'max_bin': 300, 'subsample': 0.6},\n",
       "  mean: 0.91193, std: 0.02032, params: {'colsample_bytree': 0.8, 'max_bin': 300, 'subsample': 0.7},\n",
       "  mean: 0.91237, std: 0.01957, params: {'colsample_bytree': 0.8, 'max_bin': 300, 'subsample': 0.8},\n",
       "  mean: 0.91313, std: 0.01979, params: {'colsample_bytree': 0.8, 'max_bin': 300, 'subsample': 0.9},\n",
       "  mean: 0.91356, std: 0.01930, params: {'colsample_bytree': 0.8, 'max_bin': 300, 'subsample': 1.0},\n",
       "  mean: 0.91004, std: 0.02102, params: {'colsample_bytree': 0.8, 'max_bin': 400, 'subsample': 0.5},\n",
       "  mean: 0.91160, std: 0.02025, params: {'colsample_bytree': 0.8, 'max_bin': 400, 'subsample': 0.6},\n",
       "  mean: 0.91193, std: 0.02032, params: {'colsample_bytree': 0.8, 'max_bin': 400, 'subsample': 0.7},\n",
       "  mean: 0.91237, std: 0.01957, params: {'colsample_bytree': 0.8, 'max_bin': 400, 'subsample': 0.8},\n",
       "  mean: 0.91313, std: 0.01979, params: {'colsample_bytree': 0.8, 'max_bin': 400, 'subsample': 0.9},\n",
       "  mean: 0.91356, std: 0.01930, params: {'colsample_bytree': 0.8, 'max_bin': 400, 'subsample': 1.0},\n",
       "  mean: 0.91004, std: 0.02102, params: {'colsample_bytree': 0.8, 'max_bin': 512, 'subsample': 0.5},\n",
       "  mean: 0.91160, std: 0.02025, params: {'colsample_bytree': 0.8, 'max_bin': 512, 'subsample': 0.6},\n",
       "  mean: 0.91193, std: 0.02032, params: {'colsample_bytree': 0.8, 'max_bin': 512, 'subsample': 0.7},\n",
       "  mean: 0.91237, std: 0.01957, params: {'colsample_bytree': 0.8, 'max_bin': 512, 'subsample': 0.8},\n",
       "  mean: 0.91313, std: 0.01979, params: {'colsample_bytree': 0.8, 'max_bin': 512, 'subsample': 0.9},\n",
       "  mean: 0.91356, std: 0.01930, params: {'colsample_bytree': 0.8, 'max_bin': 512, 'subsample': 1.0},\n",
       "  mean: 0.91018, std: 0.02045, params: {'colsample_bytree': 0.9, 'max_bin': 150, 'subsample': 0.5},\n",
       "  mean: 0.91211, std: 0.01933, params: {'colsample_bytree': 0.9, 'max_bin': 150, 'subsample': 0.6},\n",
       "  mean: 0.91230, std: 0.02036, params: {'colsample_bytree': 0.9, 'max_bin': 150, 'subsample': 0.7},\n",
       "  mean: 0.91310, std: 0.01905, params: {'colsample_bytree': 0.9, 'max_bin': 150, 'subsample': 0.8},\n",
       "  mean: 0.91277, std: 0.01947, params: {'colsample_bytree': 0.9, 'max_bin': 150, 'subsample': 0.9},\n",
       "  mean: 0.91233, std: 0.01984, params: {'colsample_bytree': 0.9, 'max_bin': 150, 'subsample': 1.0},\n",
       "  mean: 0.91018, std: 0.02045, params: {'colsample_bytree': 0.9, 'max_bin': 220, 'subsample': 0.5},\n",
       "  mean: 0.91211, std: 0.01933, params: {'colsample_bytree': 0.9, 'max_bin': 220, 'subsample': 0.6},\n",
       "  mean: 0.91230, std: 0.02036, params: {'colsample_bytree': 0.9, 'max_bin': 220, 'subsample': 0.7},\n",
       "  mean: 0.91310, std: 0.01905, params: {'colsample_bytree': 0.9, 'max_bin': 220, 'subsample': 0.8},\n",
       "  mean: 0.91277, std: 0.01947, params: {'colsample_bytree': 0.9, 'max_bin': 220, 'subsample': 0.9},\n",
       "  mean: 0.91233, std: 0.01984, params: {'colsample_bytree': 0.9, 'max_bin': 220, 'subsample': 1.0},\n",
       "  mean: 0.91018, std: 0.02045, params: {'colsample_bytree': 0.9, 'max_bin': 300, 'subsample': 0.5},\n",
       "  mean: 0.91211, std: 0.01933, params: {'colsample_bytree': 0.9, 'max_bin': 300, 'subsample': 0.6},\n",
       "  mean: 0.91230, std: 0.02036, params: {'colsample_bytree': 0.9, 'max_bin': 300, 'subsample': 0.7},\n",
       "  mean: 0.91310, std: 0.01905, params: {'colsample_bytree': 0.9, 'max_bin': 300, 'subsample': 0.8},\n",
       "  mean: 0.91277, std: 0.01947, params: {'colsample_bytree': 0.9, 'max_bin': 300, 'subsample': 0.9},\n",
       "  mean: 0.91233, std: 0.01984, params: {'colsample_bytree': 0.9, 'max_bin': 300, 'subsample': 1.0},\n",
       "  mean: 0.91018, std: 0.02045, params: {'colsample_bytree': 0.9, 'max_bin': 400, 'subsample': 0.5},\n",
       "  mean: 0.91211, std: 0.01933, params: {'colsample_bytree': 0.9, 'max_bin': 400, 'subsample': 0.6},\n",
       "  mean: 0.91230, std: 0.02036, params: {'colsample_bytree': 0.9, 'max_bin': 400, 'subsample': 0.7},\n",
       "  mean: 0.91310, std: 0.01905, params: {'colsample_bytree': 0.9, 'max_bin': 400, 'subsample': 0.8},\n",
       "  mean: 0.91277, std: 0.01947, params: {'colsample_bytree': 0.9, 'max_bin': 400, 'subsample': 0.9},\n",
       "  mean: 0.91233, std: 0.01984, params: {'colsample_bytree': 0.9, 'max_bin': 400, 'subsample': 1.0},\n",
       "  mean: 0.91018, std: 0.02045, params: {'colsample_bytree': 0.9, 'max_bin': 512, 'subsample': 0.5},\n",
       "  mean: 0.91211, std: 0.01933, params: {'colsample_bytree': 0.9, 'max_bin': 512, 'subsample': 0.6},\n",
       "  mean: 0.91230, std: 0.02036, params: {'colsample_bytree': 0.9, 'max_bin': 512, 'subsample': 0.7},\n",
       "  mean: 0.91310, std: 0.01905, params: {'colsample_bytree': 0.9, 'max_bin': 512, 'subsample': 0.8},\n",
       "  mean: 0.91277, std: 0.01947, params: {'colsample_bytree': 0.9, 'max_bin': 512, 'subsample': 0.9},\n",
       "  mean: 0.91233, std: 0.01984, params: {'colsample_bytree': 0.9, 'max_bin': 512, 'subsample': 1.0},\n",
       "  mean: 0.91051, std: 0.01966, params: {'colsample_bytree': 1.0, 'max_bin': 150, 'subsample': 0.5},\n",
       "  mean: 0.91142, std: 0.02043, params: {'colsample_bytree': 1.0, 'max_bin': 150, 'subsample': 0.6},\n",
       "  mean: 0.91209, std: 0.02011, params: {'colsample_bytree': 1.0, 'max_bin': 150, 'subsample': 0.7},\n",
       "  mean: 0.91185, std: 0.01948, params: {'colsample_bytree': 1.0, 'max_bin': 150, 'subsample': 0.8},\n",
       "  mean: 0.91266, std: 0.01991, params: {'colsample_bytree': 1.0, 'max_bin': 150, 'subsample': 0.9},\n",
       "  mean: 0.91151, std: 0.01919, params: {'colsample_bytree': 1.0, 'max_bin': 150, 'subsample': 1.0},\n",
       "  mean: 0.91051, std: 0.01966, params: {'colsample_bytree': 1.0, 'max_bin': 220, 'subsample': 0.5},\n",
       "  mean: 0.91142, std: 0.02043, params: {'colsample_bytree': 1.0, 'max_bin': 220, 'subsample': 0.6},\n",
       "  mean: 0.91209, std: 0.02011, params: {'colsample_bytree': 1.0, 'max_bin': 220, 'subsample': 0.7},\n",
       "  mean: 0.91185, std: 0.01948, params: {'colsample_bytree': 1.0, 'max_bin': 220, 'subsample': 0.8},\n",
       "  mean: 0.91266, std: 0.01991, params: {'colsample_bytree': 1.0, 'max_bin': 220, 'subsample': 0.9},\n",
       "  mean: 0.91151, std: 0.01919, params: {'colsample_bytree': 1.0, 'max_bin': 220, 'subsample': 1.0},\n",
       "  mean: 0.91051, std: 0.01966, params: {'colsample_bytree': 1.0, 'max_bin': 300, 'subsample': 0.5},\n",
       "  mean: 0.91142, std: 0.02043, params: {'colsample_bytree': 1.0, 'max_bin': 300, 'subsample': 0.6},\n",
       "  mean: 0.91209, std: 0.02011, params: {'colsample_bytree': 1.0, 'max_bin': 300, 'subsample': 0.7},\n",
       "  mean: 0.91185, std: 0.01948, params: {'colsample_bytree': 1.0, 'max_bin': 300, 'subsample': 0.8},\n",
       "  mean: 0.91266, std: 0.01991, params: {'colsample_bytree': 1.0, 'max_bin': 300, 'subsample': 0.9},\n",
       "  mean: 0.91151, std: 0.01919, params: {'colsample_bytree': 1.0, 'max_bin': 300, 'subsample': 1.0},\n",
       "  mean: 0.91051, std: 0.01966, params: {'colsample_bytree': 1.0, 'max_bin': 400, 'subsample': 0.5},\n",
       "  mean: 0.91142, std: 0.02043, params: {'colsample_bytree': 1.0, 'max_bin': 400, 'subsample': 0.6},\n",
       "  mean: 0.91209, std: 0.02011, params: {'colsample_bytree': 1.0, 'max_bin': 400, 'subsample': 0.7},\n",
       "  mean: 0.91185, std: 0.01948, params: {'colsample_bytree': 1.0, 'max_bin': 400, 'subsample': 0.8},\n",
       "  mean: 0.91266, std: 0.01991, params: {'colsample_bytree': 1.0, 'max_bin': 400, 'subsample': 0.9},\n",
       "  mean: 0.91151, std: 0.01919, params: {'colsample_bytree': 1.0, 'max_bin': 400, 'subsample': 1.0},\n",
       "  mean: 0.91051, std: 0.01966, params: {'colsample_bytree': 1.0, 'max_bin': 512, 'subsample': 0.5},\n",
       "  mean: 0.91142, std: 0.02043, params: {'colsample_bytree': 1.0, 'max_bin': 512, 'subsample': 0.6},\n",
       "  mean: 0.91209, std: 0.02011, params: {'colsample_bytree': 1.0, 'max_bin': 512, 'subsample': 0.7},\n",
       "  mean: 0.91185, std: 0.01948, params: {'colsample_bytree': 1.0, 'max_bin': 512, 'subsample': 0.8},\n",
       "  mean: 0.91266, std: 0.01991, params: {'colsample_bytree': 1.0, 'max_bin': 512, 'subsample': 0.9},\n",
       "  mean: 0.91151, std: 0.01919, params: {'colsample_bytree': 1.0, 'max_bin': 512, 'subsample': 1.0}],\n",
       " {'colsample_bytree': 0.8, 'max_bin': 150, 'subsample': 1.0},\n",
       " 0.9135565816465017)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finetuning the number of leaves\n",
    "params = {'n_estimators':80, 'max_depth' : -1, 'objective': 'binary', 'num_leaves': 96, 'learning_rate': 0.05, \n",
    "          'max_bin': 512, 'subsample_for_bin': 200, 'subsample': 1, 'subsample_freq': 1, 'colsample_bytree': 0.8, \n",
    "          'reg_alpha': 5, 'reg_lambda': 10, 'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 5, \n",
    "          'class_weight': 'balanced', 'num_class' : 1, 'metric' : 'binary_error', 'min_data_in_leaf':2}\n",
    "\n",
    "gridParams = {\n",
    "    'max_bin': [150, 220, 300, 400, 512],\n",
    "    'subsample': [x/10 for x in range(5, 11)],\n",
    "    'colsample_bytree': [x/10 for x in range(5, 11)],\n",
    "    }\n",
    "\n",
    "mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', \n",
    "          n_jobs = 5, # Updated from 'nthread' \n",
    "          silent = True,\n",
    "          **params)\n",
    "grid = GridSearchCV(estimator=mdl, param_grid = gridParams, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=10 )\n",
    "\n",
    "grid.fit(X_vect,y)#, categorical_feature=range(X_vect.shape[1]))\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning regularization parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.91628, std: 0.01757, params: {'min_split_gain': 0.01},\n",
       "  mean: 0.91606, std: 0.01747, params: {'min_split_gain': 0.05},\n",
       "  mean: 0.91712, std: 0.01713, params: {'min_split_gain': 0.1}],\n",
       " {'min_split_gain': 0.1},\n",
       " 0.9171226353707322)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finetuning the number of leaves\n",
    "params = {'n_estimators':80, 'max_depth' : -1, 'objective': 'binary', 'num_leaves': 96, 'learning_rate': 0.05, \n",
    "          'max_bin': 150, 'subsample_for_bin': 200, 'subsample': 1, 'subsample_freq': 1, 'colsample_bytree': 0.8, \n",
    "          'reg_alpha': .1, 'reg_lambda': 1, 'min_split_gain': 0.5, 'min_child_weight': 1, 'min_child_samples': 5, \n",
    "          'class_weight': 'balanced', 'num_class' : 1, 'metric' : 'binary_error', 'min_data_in_leaf':2}\n",
    "\n",
    "gridParams = {\n",
    "    #'reg_alpha': [.01, .1, 1, 10, 100],\n",
    "    #'reg_lambda': [.01, .1, 1, 10, 100],\n",
    "    'min_split_gain': [.01, .05, .1]\n",
    "    }\n",
    "\n",
    "mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', \n",
    "          n_jobs = 5, # Updated from 'nthread' \n",
    "          silent = True,\n",
    "          **params)\n",
    "grid = GridSearchCV(estimator=mdl, param_grid = gridParams, scoring='roc_auc',n_jobs=1,iid=False, \n",
    "                    cv=10 )\n",
    "\n",
    "grid.fit(X_vect,y)#, categorical_feature=range(X_vect.shape[1]))\n",
    "grid.grid_scores_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retuning numberof trees with new found parameters and lower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franck\\Anaconda3\\envs\\nlp\\lib\\site-packages\\lightgbm\\engine.py:390: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tcv_agg's binary_logloss: 0.679053 + 0.00119989\tcv_agg's auc: 0.797744 + 0.0254859\n",
      "[2]\tcv_agg's binary_logloss: 0.662025 + 0.00200461\tcv_agg's auc: 0.857955 + 0.0209377\n",
      "[3]\tcv_agg's binary_logloss: 0.645534 + 0.00292012\tcv_agg's auc: 0.869148 + 0.0203439\n",
      "[4]\tcv_agg's binary_logloss: 0.631154 + 0.00384208\tcv_agg's auc: 0.874236 + 0.0201445\n",
      "[5]\tcv_agg's binary_logloss: 0.617221 + 0.00471959\tcv_agg's auc: 0.875265 + 0.019612\n",
      "[6]\tcv_agg's binary_logloss: 0.605083 + 0.00564029\tcv_agg's auc: 0.876878 + 0.0191486\n",
      "[7]\tcv_agg's binary_logloss: 0.596661 + 0.00575039\tcv_agg's auc: 0.878984 + 0.0184191\n",
      "[8]\tcv_agg's binary_logloss: 0.585463 + 0.00630052\tcv_agg's auc: 0.879395 + 0.0174919\n",
      "[9]\tcv_agg's binary_logloss: 0.575347 + 0.00699481\tcv_agg's auc: 0.879574 + 0.0175068\n",
      "[10]\tcv_agg's binary_logloss: 0.566443 + 0.00789113\tcv_agg's auc: 0.88062 + 0.0181459\n",
      "[11]\tcv_agg's binary_logloss: 0.557513 + 0.00845265\tcv_agg's auc: 0.88154 + 0.017927\n",
      "[12]\tcv_agg's binary_logloss: 0.551682 + 0.0089464\tcv_agg's auc: 0.88303 + 0.0182703\n",
      "[13]\tcv_agg's binary_logloss: 0.543697 + 0.0097658\tcv_agg's auc: 0.883505 + 0.0183506\n",
      "[14]\tcv_agg's binary_logloss: 0.536484 + 0.0103856\tcv_agg's auc: 0.883574 + 0.0185728\n",
      "[15]\tcv_agg's binary_logloss: 0.531007 + 0.0111343\tcv_agg's auc: 0.884076 + 0.0188622\n",
      "[16]\tcv_agg's binary_logloss: 0.524251 + 0.0116747\tcv_agg's auc: 0.884486 + 0.0189219\n",
      "[17]\tcv_agg's binary_logloss: 0.517385 + 0.0122639\tcv_agg's auc: 0.88454 + 0.0188043\n",
      "[18]\tcv_agg's binary_logloss: 0.511358 + 0.0129628\tcv_agg's auc: 0.884408 + 0.0188251\n",
      "[19]\tcv_agg's binary_logloss: 0.507361 + 0.0131844\tcv_agg's auc: 0.885359 + 0.0187119\n",
      "[20]\tcv_agg's binary_logloss: 0.50201 + 0.0136453\tcv_agg's auc: 0.886091 + 0.0185298\n",
      "[21]\tcv_agg's binary_logloss: 0.496976 + 0.0141184\tcv_agg's auc: 0.886017 + 0.0184916\n",
      "[22]\tcv_agg's binary_logloss: 0.492009 + 0.0145519\tcv_agg's auc: 0.886491 + 0.0181677\n",
      "[23]\tcv_agg's binary_logloss: 0.487767 + 0.0151123\tcv_agg's auc: 0.886871 + 0.0182933\n",
      "[24]\tcv_agg's binary_logloss: 0.483582 + 0.0155324\tcv_agg's auc: 0.886818 + 0.0181447\n",
      "[25]\tcv_agg's binary_logloss: 0.479745 + 0.0160578\tcv_agg's auc: 0.886734 + 0.0182487\n",
      "[26]\tcv_agg's binary_logloss: 0.476836 + 0.0164472\tcv_agg's auc: 0.887627 + 0.0182557\n",
      "[27]\tcv_agg's binary_logloss: 0.47352 + 0.0167448\tcv_agg's auc: 0.887834 + 0.0181809\n",
      "[28]\tcv_agg's binary_logloss: 0.470313 + 0.0170119\tcv_agg's auc: 0.887603 + 0.0180646\n",
      "[29]\tcv_agg's binary_logloss: 0.467078 + 0.017414\tcv_agg's auc: 0.88786 + 0.0179005\n",
      "[30]\tcv_agg's binary_logloss: 0.463841 + 0.0178287\tcv_agg's auc: 0.888301 + 0.0179646\n",
      "[31]\tcv_agg's binary_logloss: 0.461529 + 0.0181409\tcv_agg's auc: 0.889035 + 0.0179546\n",
      "[32]\tcv_agg's binary_logloss: 0.458517 + 0.0186672\tcv_agg's auc: 0.889551 + 0.0180649\n",
      "[33]\tcv_agg's binary_logloss: 0.455665 + 0.0191099\tcv_agg's auc: 0.889988 + 0.0180587\n",
      "[34]\tcv_agg's binary_logloss: 0.4527 + 0.0196496\tcv_agg's auc: 0.89076 + 0.018164\n",
      "[35]\tcv_agg's binary_logloss: 0.450267 + 0.0198704\tcv_agg's auc: 0.891078 + 0.0179074\n",
      "[36]\tcv_agg's binary_logloss: 0.448108 + 0.0203399\tcv_agg's auc: 0.891192 + 0.0179793\n",
      "[37]\tcv_agg's binary_logloss: 0.446117 + 0.0206286\tcv_agg's auc: 0.891837 + 0.0179977\n",
      "[38]\tcv_agg's binary_logloss: 0.44385 + 0.0209698\tcv_agg's auc: 0.89224 + 0.0180093\n",
      "[39]\tcv_agg's binary_logloss: 0.442008 + 0.0212244\tcv_agg's auc: 0.89299 + 0.0180213\n",
      "[40]\tcv_agg's binary_logloss: 0.439886 + 0.0217718\tcv_agg's auc: 0.893308 + 0.0181236\n",
      "[41]\tcv_agg's binary_logloss: 0.438309 + 0.0219663\tcv_agg's auc: 0.893745 + 0.0180537\n",
      "[42]\tcv_agg's binary_logloss: 0.436426 + 0.022501\tcv_agg's auc: 0.894144 + 0.0181642\n",
      "[43]\tcv_agg's binary_logloss: 0.434649 + 0.0229621\tcv_agg's auc: 0.894456 + 0.018197\n",
      "[44]\tcv_agg's binary_logloss: 0.432891 + 0.0232894\tcv_agg's auc: 0.894791 + 0.0182728\n",
      "[45]\tcv_agg's binary_logloss: 0.430947 + 0.02356\tcv_agg's auc: 0.89553 + 0.0181836\n",
      "[46]\tcv_agg's binary_logloss: 0.429484 + 0.0239164\tcv_agg's auc: 0.895733 + 0.018205\n",
      "[47]\tcv_agg's binary_logloss: 0.427978 + 0.0241801\tcv_agg's auc: 0.895965 + 0.0181373\n",
      "[48]\tcv_agg's binary_logloss: 0.426493 + 0.0245589\tcv_agg's auc: 0.896269 + 0.0182324\n",
      "[49]\tcv_agg's binary_logloss: 0.424969 + 0.024884\tcv_agg's auc: 0.896721 + 0.0181954\n",
      "[50]\tcv_agg's binary_logloss: 0.423866 + 0.025145\tcv_agg's auc: 0.897039 + 0.0181881\n",
      "[51]\tcv_agg's binary_logloss: 0.4227 + 0.0255505\tcv_agg's auc: 0.897433 + 0.0183524\n",
      "[52]\tcv_agg's binary_logloss: 0.421627 + 0.0258031\tcv_agg's auc: 0.897686 + 0.0184062\n",
      "[53]\tcv_agg's binary_logloss: 0.420386 + 0.0261442\tcv_agg's auc: 0.898021 + 0.0184206\n",
      "[54]\tcv_agg's binary_logloss: 0.419214 + 0.0265808\tcv_agg's auc: 0.898381 + 0.0185473\n",
      "[55]\tcv_agg's binary_logloss: 0.418126 + 0.0269925\tcv_agg's auc: 0.898564 + 0.0186093\n",
      "[56]\tcv_agg's binary_logloss: 0.417147 + 0.0272754\tcv_agg's auc: 0.898826 + 0.0185625\n",
      "[57]\tcv_agg's binary_logloss: 0.416149 + 0.0275081\tcv_agg's auc: 0.899139 + 0.0185288\n",
      "[58]\tcv_agg's binary_logloss: 0.415094 + 0.0278035\tcv_agg's auc: 0.899408 + 0.0185967\n",
      "[59]\tcv_agg's binary_logloss: 0.414185 + 0.0279674\tcv_agg's auc: 0.899681 + 0.0185243\n",
      "[60]\tcv_agg's binary_logloss: 0.413486 + 0.0282347\tcv_agg's auc: 0.899771 + 0.0185234\n",
      "[61]\tcv_agg's binary_logloss: 0.412745 + 0.0285259\tcv_agg's auc: 0.90004 + 0.0186013\n",
      "[62]\tcv_agg's binary_logloss: 0.41192 + 0.0287963\tcv_agg's auc: 0.90029 + 0.0186405\n",
      "[63]\tcv_agg's binary_logloss: 0.411337 + 0.0291173\tcv_agg's auc: 0.900359 + 0.0186846\n",
      "[64]\tcv_agg's binary_logloss: 0.410683 + 0.0291884\tcv_agg's auc: 0.900596 + 0.0185202\n",
      "[65]\tcv_agg's binary_logloss: 0.410065 + 0.0294398\tcv_agg's auc: 0.900691 + 0.0185545\n",
      "[66]\tcv_agg's binary_logloss: 0.40946 + 0.0297442\tcv_agg's auc: 0.900784 + 0.0185712\n",
      "[67]\tcv_agg's binary_logloss: 0.40881 + 0.0299864\tcv_agg's auc: 0.901108 + 0.0185428\n",
      "[68]\tcv_agg's binary_logloss: 0.408276 + 0.0300367\tcv_agg's auc: 0.901241 + 0.0185002\n",
      "[69]\tcv_agg's binary_logloss: 0.40775 + 0.0301739\tcv_agg's auc: 0.901399 + 0.0184304\n",
      "[70]\tcv_agg's binary_logloss: 0.407184 + 0.0303985\tcv_agg's auc: 0.901633 + 0.0184792\n",
      "[71]\tcv_agg's binary_logloss: 0.406881 + 0.0304815\tcv_agg's auc: 0.901698 + 0.0184081\n",
      "[72]\tcv_agg's binary_logloss: 0.406457 + 0.0305494\tcv_agg's auc: 0.90182 + 0.0183169\n",
      "[73]\tcv_agg's binary_logloss: 0.406021 + 0.0308703\tcv_agg's auc: 0.901958 + 0.0183946\n",
      "[74]\tcv_agg's binary_logloss: 0.405586 + 0.0310879\tcv_agg's auc: 0.902109 + 0.0184007\n",
      "[75]\tcv_agg's binary_logloss: 0.405067 + 0.031364\tcv_agg's auc: 0.902256 + 0.0184393\n",
      "[76]\tcv_agg's binary_logloss: 0.404692 + 0.0316061\tcv_agg's auc: 0.902375 + 0.0184685\n",
      "[77]\tcv_agg's binary_logloss: 0.404339 + 0.0317342\tcv_agg's auc: 0.902451 + 0.0184543\n",
      "[78]\tcv_agg's binary_logloss: 0.40383 + 0.0318448\tcv_agg's auc: 0.902695 + 0.0183926\n",
      "[79]\tcv_agg's binary_logloss: 0.403515 + 0.0320805\tcv_agg's auc: 0.902727 + 0.0184203\n",
      "[80]\tcv_agg's binary_logloss: 0.403452 + 0.0323795\tcv_agg's auc: 0.90264 + 0.0185316\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators':80, 'max_depth' : -1, 'objective': 'binary', 'num_leaves': 96, 'learning_rate': 0.05, \n",
    "          'max_bin': 150, 'subsample_for_bin': 200, 'subsample': 1, 'subsample_freq': 1, 'colsample_bytree': 0.8, \n",
    "          'reg_alpha': .1, 'reg_lambda': 1, 'min_split_gain': 0.1, 'min_child_weight': 1, 'min_child_samples': 5, \n",
    "          'class_weight': 'balanced', 'num_class' : 1, 'metric' : 'binary_error', 'min_data_in_leaf':2}\n",
    "pos_weight = 1-np.mean(y)\n",
    "weight = y.apply(lambda x : pos_weight if x == 1 else 1 - pos_weight)\n",
    "dataset = lgb.Dataset(data=X_vect, label=y, weight=weight)\n",
    "cv = lgb.cv(params=params, train_set=dataset, num_boost_round=1000, nfold=10, stratified=True,\n",
    "                early_stopping_rounds=10, metrics=['binary', 'auc'], verbose_eval=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noticed that for some reason the LGB crossvalidation framework gives me worse results than the scikit-learn one. This would be work investigating in a challenge setting for example but I decided to try not to spend to much time on this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking\n",
    "Finally, reviewing the state-of-the-art model recommended by people for Kaggle challenges and such, I noticed that stacking different models brought significant boost in performance more often that not. Since I had already build a bunh of models, I decided to stack them. \n",
    "The philosophy behind stacking is a little bit involved but one of the main takeway is that we are going to use the prediction of ours models as features to build a second model. However, since learning the 1st level models the usual way would make use of dependent variables, inducing a huge information leakage, we are going the make out of fold predictions using cross validation for each of the 1st model. We will then train our 2nd layer model on those out-of-fold predictions. We are also  going to train 1st layer models on the whole training set but we are going to use them to predict the 2nd layer features of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vect,y, stratify=y)\n",
    "\n",
    "columns = ['SVM', 'Boost', 'Logit', 'LGB']\n",
    "#columns = ['Logit', 'Boost', 'LGB']\n",
    "train_meta = pd.DataFrame(np.nan, index=range(X_train.shape[0]), columns=columns)\n",
    "test_meta = pd.DataFrame(np.nan, index=range(X_test.shape[0]), columns=columns)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, random_state=12345, shuffle=True)\n",
    "for sub_train_index, sub_val_index in skf.split(X_train, y_train):\n",
    "    X_train_sub = X_train[sub_train_index,:]\n",
    "    y_train_sub = y_train.iloc[sub_train_index]\n",
    "    X_val_sub = X_train[sub_val_index,:]\n",
    "    y_val_sub = y_train.iloc[sub_val_index]\n",
    "    \n",
    "    \n",
    "    svm = SVC(C=10, gamma=8/X_vect.shape[1], class_weight='balanced', probability=True)\n",
    "    svm.fit(X_train_sub, y_train_sub)\n",
    "    train_meta['SVM'][sub_val_index] = svm.predict_proba(X_val_sub)[:,1].flatten()\n",
    "    test_meta['SVM'] = svm.predict_proba(X_test)[:,:1].flatten()\n",
    "    \n",
    "    \n",
    "    logit = LogisticRegression(C=1, class_weight='balanced')\n",
    "    logit.fit(X_train_sub, y_train_sub)\n",
    "    train_meta['Logit'][sub_val_index] = logit.predict_proba(X_val_sub)[:,1].flatten()\n",
    "    test_meta['Logit'] = logit.predict_proba(X_test)[:,1].flatten()\n",
    "    \n",
    "    \n",
    "    boost = xgb.XGBClassifier( learning_rate =0.1, n_estimators=109, max_depth=14,\n",
    "     min_child_weight=13, gamma=20, subsample=0.9, colsample_bytree=0.4,\n",
    "     objective= 'binary:logistic', nthread=1, seed=27, scale_pos_weight = (1 - np.mean(y))/np.mean(y)\n",
    "                             )\n",
    "    boost.fit(X_train_sub, y_train_sub)\n",
    "    train_meta['Boost'][sub_val_index] = boost.predict_proba(X_val_sub)[:,1].flatten()\n",
    "    test_meta['Boost'] = boost.predict_proba(X_test)[:,1].flatten()         \n",
    "    \n",
    "    params = {'n_estimators':80, 'max_depth' : -1, 'objective': 'binary', 'num_leaves': 96, 'learning_rate': 0.05, \n",
    "          'max_bin': 150, 'subsample_for_bin': 200, 'subsample': 1, 'subsample_freq': 1, 'colsample_bytree': 0.8, \n",
    "          'reg_alpha': .1, 'reg_lambda': 1, 'min_split_gain': 0.1, 'min_child_weight': 1, 'min_child_samples': 5, \n",
    "          'class_weight': 'balanced', 'num_class' : 1, 'metric' : 'binary_error', 'min_data_in_leaf':2}\n",
    "    mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', \n",
    "          n_jobs = 5, # Updated from 'nthread' \n",
    "          silent = True,\n",
    "          **params)\n",
    "    mdl.fit(X_train_sub, y_train_sub)\n",
    "    train_meta['LGB'][sub_val_index] = mdl.predict_proba(X_val_sub)[:,1].flatten()\n",
    "    test_meta['LGB'] = mdl.predict_proba(X_test)[:,1].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_svm = SVC(C=10, gamma=8/X_vect.shape[1], class_weight='balanced', probability=True)\n",
    "final_svm.fit(X_train, y_train)\n",
    "test_meta['SVM'] = final_svm.predict_proba(X_test)[:,:1].flatten()\n",
    "\n",
    "\n",
    "final_logit = LogisticRegression(C=1, class_weight='balanced')\n",
    "final_logit.fit(X_train, y_train)\n",
    "test_meta['Logit'] = final_logit.predict_proba(X_test)[:,1].flatten()\n",
    "\n",
    "\n",
    "final_boost = xgb.XGBClassifier( learning_rate =0.1, n_estimators=109, max_depth=14,\n",
    " min_child_weight=13, gamma=20, subsample=0.9, colsample_bytree=0.4,\n",
    " objective= 'binary:logistic', nthread=1, seed=27, scale_pos_weight = (1 - np.mean(y))/np.mean(y)\n",
    "                         )\n",
    "final_boost.fit(X_train, y_train)\n",
    "test_meta['Boost'] = final_boost.predict_proba(X_test)[:,1].flatten()         \n",
    "\n",
    "params = {'n_estimators':80, 'max_depth' : -1, 'objective': 'binary', 'num_leaves': 96, 'learning_rate': 0.05, \n",
    "      'max_bin': 150, 'subsample_for_bin': 200, 'subsample': 1, 'subsample_freq': 1, 'colsample_bytree': 0.8, \n",
    "      'reg_alpha': .1, 'reg_lambda': 1, 'min_split_gain': 0.1, 'min_child_weight': 1, 'min_child_samples': 5, \n",
    "      'class_weight': 'balanced', 'num_class' : 1, 'metric' : 'binary_error', 'min_data_in_leaf':2}\n",
    "final_mdl = lgb.LGBMClassifier(boosting_type= 'gbdt', \n",
    "      n_jobs = 5, # Updated from 'nthread' \n",
    "      silent = True,\n",
    "      **params)\n",
    "final_mdl.fit(X_train, y_train)\n",
    "test_meta['LGB'] = final_mdl.predict_proba(X_test)[:,1].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc auc score Score: 0.9294674191406621\n",
      "roc auc score Score for Boosting: 0.9247912566394765\n"
     ]
    }
   ],
   "source": [
    "final_estimator = LogisticRegressionCV(Cs=[.1, 1, 10], class_weight='balanced')\n",
    "final_estimator.fit(train_meta, y_train)\n",
    "final_prob = final_estimator.predict_proba(test_meta)[:,1].flatten()\n",
    "\n",
    "\n",
    "final_boost_prob = final_boost.predict_proba(X_test)[:,1].flatten()\n",
    "roc_auc = roc_auc_score(y_test, final_prob)\n",
    "roc_auc_boost = roc_auc_score(y_test, final_boost_prob)\n",
    "print('roc auc score Score: {}'.format(roc_auc))\n",
    "print('roc auc score Score for Boosting: {}'.format(roc_auc_boost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that the stacking provides a slight boost in the roc auc score. We would need to do another layer of crossvalidation to have a better estimation but given the rather small sample size this might prove unstable. Altough this might not seem like much, every littleimprovement might make a difference in settings like Kaggle competitions.\n",
    "Finally, we can pickle the model (the models actually since we are going to use the stacked model) using scikit-learn joblib) to allow our command line program to call it to make it's predictions.\n",
    "Since we did not include the preprocessing steps in the pipeline for the parametertuning, we are going to save each stepas it's own model and rebuild the pipeline from them at prediction time (without refitting it obviously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../saved_models/final_estimator.pkl']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib.dump(pipe.named_steps['processor'], '../saved_models/processor.pkl')\n",
    "#joblib.dump(pipe.named_steps['vect'], '../saved_models/vect.pkl')\n",
    "joblib.dump(pipe, '../saved_models/pipe.pkl')\n",
    "joblib.dump(final_svm, '../saved_models/final_svm.pkl')\n",
    "joblib.dump(final_logit, '../saved_models/final_logit.pkl')\n",
    "joblib.dump(final_mdl, '../saved_models/final_LGB.pkl')\n",
    "joblib.dump(final_boost, '../saved_models/final_boost.pkl')\n",
    "joblib.dump(final_estimator, '../saved_models/final_estimator.pkl')\n",
    "\n",
    "                             "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
